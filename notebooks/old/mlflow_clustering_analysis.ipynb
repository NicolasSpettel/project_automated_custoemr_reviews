{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c84d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from transformers import pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_features(row: pd.Series) -> str:\n",
    "    text = \"\"\n",
    "    name = row.get(\"name\", \"\")\n",
    "    if pd.notna(name) and str(name) != \"nan\":\n",
    "        text = str(name)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_clusters(df: pd.DataFrame, cluster_col: str, text_col: str) -> dict:\n",
    "    names = {}\n",
    "    for cid in sorted(df[cluster_col].unique()):\n",
    "        all_text = \" \".join(df.loc[df[cluster_col] == cid, text_col].tolist())\n",
    "        word_counts = Counter(re.findall(r\"\\b\\w{3,}\\b\", all_text))\n",
    "        top_words = [w for w, _ in word_counts.most_common(3)]\n",
    "        cname = \" \".join(top_words).title() if top_words else f\"Product Group {cid + 1}\"\n",
    "        names[cid] = cname\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_stats(df: pd.DataFrame, cluster_col: str) -> pd.DataFrame:\n",
    "    stats_rows = []\n",
    "    for cid, g in df.groupby(cluster_col):\n",
    "        row = {\n",
    "            \"cluster_id\": cid,\n",
    "            \"cluster_name\": g[\"cluster_name\"].iloc[0],\n",
    "            \"count\": len(g),\n",
    "            \"avg_sentiment\": g[\"predicted_sentiment_roberta\"].mean(),\n",
    "            \"positive_pct\": (g[\"predicted_sentiment_roberta\"] > 0.5).mean() * 100,\n",
    "        }\n",
    "        if \"rating\" in g.columns:\n",
    "            row[\"avg_rating\"] = g[\"rating\"].mean()\n",
    "            row[\"high_rating_pct\"] = (g[\"rating\"] >= 4).mean() * 100\n",
    "        if \"doRecommend\" in g.columns:\n",
    "            row[\"recommend_pct\"] = g[\"doRecommend\"].mean() * 100\n",
    "        stats_rows.append(row)\n",
    "    return pd.DataFrame(stats_rows).sort_values(\"count\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4562e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_category_stats(df: pd.DataFrame, cat_col: str) -> pd.DataFrame:\n",
    "    stats_rows = []\n",
    "    for cat, g in df.groupby(cat_col):\n",
    "        row = {\n",
    "            \"category\": cat,\n",
    "            \"count\": len(g),\n",
    "            \"avg_sentiment\": g[\"predicted_sentiment_roberta\"].mean(),\n",
    "            \"positive_pct\": (g[\"predicted_sentiment_roberta\"] > 0.5).mean() * 100,\n",
    "            \"avg_zero_shot_score\": g[\"zero_shot_score\"].mean() if \"zero_shot_score\" in g.columns else np.nan,\n",
    "        }\n",
    "        if \"rating\" in g.columns:\n",
    "            row[\"avg_rating\"] = g[\"rating\"].mean()\n",
    "            row[\"high_rating_pct\"] = (g[\"rating\"] >= 4).mean() * 100\n",
    "        if \"doRecommend\" in g.columns:\n",
    "            row[\"recommend_pct\"] = g[\"doRecommend\"].mean() * 100\n",
    "        stats_rows.append(row)\n",
    "    return pd.DataFrame(stats_rows).sort_values(\"count\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_batch(\n",
    "    texts: list[str],\n",
    "    labels: list[str],\n",
    "    model_name: str = \"facebook/bart-large-mnli\",\n",
    "    batch_size: int = 16,\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    zsc = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "    preds, scores = [], []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        out = zsc(batch, candidate_labels=labels, multi_label=False)\n",
    "        if isinstance(out, dict):\n",
    "            out = [out]\n",
    "        for o in out:\n",
    "            preds.append(o[\"labels\"][0])\n",
    "            scores.append(float(o[\"scores\"][0]))\n",
    "    return preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "975b3e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed94e037c0e402e801490a6ed03e023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6d357ebad9421c9694a84bb12d2cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670ea78c522045fabe70c0f396bd4d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de507bc834204d71a5f052b1f169ae7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e591f839c443ffb7c00eeacf2abdc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740e64a7e68949cd9477cafd6622a510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb78ce215ad848d39809578ab01957e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7be46901b474d289d1969bf1699f302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f62cd123df54a269d37240807eb0ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a234f5b8f6744fabd83d6c5bf1327cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0db97b9c22413abaada713b5d891c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/27 12:04:02 INFO mlflow.tracking.fluent: Experiment with name 'Product Clustering & Sentiment Analysis' does not exist. Creating a new experiment.\n",
      "2025/08/27 12:04:03 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/27 12:04:07 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411196f2a4cb4b7d823bb1a3b637fd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98db317046140cca8c6584dea9d94b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801305e608da4e12bb7c16b78a58ad22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e5f10625cf441c8ac76da65e06862f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60976939b1c4aa296b30b5e46c4a466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2e2aa45854484080af4088e15671ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    105\u001b[39m         mlflow.log_artifact(\u001b[33m\"\u001b[39m\u001b[33mzero_shot_stats.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     61\u001b[39m     mlflow.log_artifact(\u001b[33m\"\u001b[39m\u001b[33mcluster_stats.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m zero_shot_labels = [\n\u001b[32m     64\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFire Tablet Special\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     65\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAmazonBasics Performance Alkaline\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     69\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFire Amazon With\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     70\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m zs_preds, zs_scores = zero_shot_batch(\n\u001b[32m     73\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mproduct_name_clean\u001b[39m\u001b[33m\"\u001b[39m].tolist(),\n\u001b[32m     74\u001b[39m     labels=zero_shot_labels,\n\u001b[32m     75\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33mfacebook/bart-large-mnli\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     76\u001b[39m     batch_size=\u001b[32m16\u001b[39m,\n\u001b[32m     77\u001b[39m )\n\u001b[32m     78\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mzero_shot_label\u001b[39m\u001b[33m\"\u001b[39m] = zs_preds\n\u001b[32m     79\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mzero_shot_score\u001b[39m\u001b[33m\"\u001b[39m] = zs_scores\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mzero_shot_batch\u001b[39m\u001b[34m(texts, labels, model_name, batch_size)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size):\n\u001b[32m     69\u001b[39m     batch = texts[i : i + batch_size]\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     out = zsc(batch, candidate_labels=labels, multi_label=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m     72\u001b[39m         out = [out]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:206\u001b[39m, in \u001b[36mZeroShotClassificationPipeline.__call__\u001b[39m\u001b[34m(self, sequences, *args, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(sequences, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\transformers\\pipelines\\base.py:1360\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1357\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1358\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1359\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m     outputs = \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001b[39m, in \u001b[36mPipelinePackIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    266\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     processed = \u001b[38;5;28mself\u001b[39m.infer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator), **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    271\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch.Tensor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\transformers\\pipelines\\base.py:1286\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1285\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._forward(model_inputs, **forward_params)\n\u001b[32m   1287\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:229\u001b[39m, in \u001b[36mZeroShotClassificationPipeline._forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(model_forward).parameters.keys():\n\u001b[32m    228\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model(**model_inputs)\n\u001b[32m    231\u001b[39m model_outputs = {\n\u001b[32m    232\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcandidate_label\u001b[39m\u001b[33m\"\u001b[39m: candidate_label,\n\u001b[32m    233\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msequence\u001b[39m\u001b[33m\"\u001b[39m: sequence,\n\u001b[32m    234\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mis_last\u001b[39m\u001b[33m\"\u001b[39m: inputs[\u001b[33m\"\u001b[39m\u001b[33mis_last\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    235\u001b[39m     **outputs,\n\u001b[32m    236\u001b[39m }\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1775\u001b[39m, in \u001b[36mBartForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1770\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1772\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPassing input embeddings is currently not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1773\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m   1776\u001b[39m     input_ids,\n\u001b[32m   1777\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   1778\u001b[39m     decoder_input_ids=decoder_input_ids,\n\u001b[32m   1779\u001b[39m     decoder_attention_mask=decoder_attention_mask,\n\u001b[32m   1780\u001b[39m     head_mask=head_mask,\n\u001b[32m   1781\u001b[39m     decoder_head_mask=decoder_head_mask,\n\u001b[32m   1782\u001b[39m     cross_attn_head_mask=cross_attn_head_mask,\n\u001b[32m   1783\u001b[39m     encoder_outputs=encoder_outputs,\n\u001b[32m   1784\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m   1785\u001b[39m     decoder_inputs_embeds=decoder_inputs_embeds,\n\u001b[32m   1786\u001b[39m     use_cache=use_cache,\n\u001b[32m   1787\u001b[39m     output_attentions=output_attentions,\n\u001b[32m   1788\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m   1789\u001b[39m     return_dict=return_dict,\n\u001b[32m   1790\u001b[39m )\n\u001b[32m   1791\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# last hidden state\u001b[39;00m\n\u001b[32m   1793\u001b[39m eos_mask = input_ids.eq(\u001b[38;5;28mself\u001b[39m.config.eos_token_id).to(hidden_states.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1533\u001b[39m, in \u001b[36mBartModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1526\u001b[39m     encoder_outputs = BaseModelOutput(\n\u001b[32m   1527\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1528\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1529\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1530\u001b[39m     )\n\u001b[32m   1532\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1533\u001b[39m decoder_outputs = \u001b[38;5;28mself\u001b[39m.decoder(\n\u001b[32m   1534\u001b[39m     input_ids=decoder_input_ids,\n\u001b[32m   1535\u001b[39m     attention_mask=decoder_attention_mask,\n\u001b[32m   1536\u001b[39m     encoder_hidden_states=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1537\u001b[39m     encoder_attention_mask=attention_mask,\n\u001b[32m   1538\u001b[39m     head_mask=decoder_head_mask,\n\u001b[32m   1539\u001b[39m     cross_attn_head_mask=cross_attn_head_mask,\n\u001b[32m   1540\u001b[39m     past_key_values=past_key_values,\n\u001b[32m   1541\u001b[39m     inputs_embeds=decoder_inputs_embeds,\n\u001b[32m   1542\u001b[39m     use_cache=use_cache,\n\u001b[32m   1543\u001b[39m     output_attentions=output_attentions,\n\u001b[32m   1544\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m   1545\u001b[39m     return_dict=return_dict,\n\u001b[32m   1546\u001b[39m )\n\u001b[32m   1548\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1549\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs + encoder_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1378\u001b[39m, in \u001b[36mBartDecoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1365\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1366\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1367\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1375\u001b[39m         use_cache,\n\u001b[32m   1376\u001b[39m     )\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m     layer_outputs = decoder_layer(\n\u001b[32m   1379\u001b[39m         hidden_states,\n\u001b[32m   1380\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   1381\u001b[39m         encoder_hidden_states=encoder_hidden_states,\n\u001b[32m   1382\u001b[39m         encoder_attention_mask=encoder_attention_mask,\n\u001b[32m   1383\u001b[39m         layer_head_mask=(head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1384\u001b[39m         cross_attn_layer_head_mask=(\n\u001b[32m   1385\u001b[39m             cross_attn_head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m cross_attn_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1386\u001b[39m         ),\n\u001b[32m   1387\u001b[39m         past_key_value=past_key_value,\n\u001b[32m   1388\u001b[39m         output_attentions=output_attentions,\n\u001b[32m   1389\u001b[39m         use_cache=use_cache,\n\u001b[32m   1390\u001b[39m     )\n\u001b[32m   1391\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:702\u001b[39m, in \u001b[36mBartDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[39m\n\u001b[32m    700\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.activation_fn(\u001b[38;5;28mself\u001b[39m.fc1(hidden_states))\n\u001b[32m    701\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.activation_dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.fc2(hidden_states)\n\u001b[32m    703\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m    704\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\anaconda3\\envs\\project_nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    file_path = \"../data/processed/Datafiniti_with_sentiments.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df[\"product_name_clean\"] = df.apply(create_text_features, axis=1)\n",
    "    df = df[df[\"product_name_clean\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "    sentiment_mapping = {\"Positive\": 1.0, \"Neutral\": 0.5, \"Negative\": 0.0}\n",
    "    if \"predicted_sentiment_roberta\" in df.columns:\n",
    "        df[\"predicted_sentiment_roberta\"] = df[\"predicted_sentiment_roberta\"].map(sentiment_mapping)\n",
    "    else:\n",
    "        df[\"predicted_sentiment_roberta\"] = np.nan\n",
    "\n",
    "    embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "    model = SentenceTransformer(embedding_model_name)\n",
    "    text_features = model.encode(df[\"product_name_clean\"].tolist(), show_progress_bar=False)\n",
    "\n",
    "    k_range = range(4, 7)\n",
    "    silhouette_scores = []\n",
    "    for k in k_range:\n",
    "        km = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "        cl = km.fit_predict(text_features)\n",
    "        silhouette_scores.append(silhouette_score(text_features, cl))\n",
    "    optimal_k = list(k_range)[int(np.argmax(silhouette_scores))]\n",
    "\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10, max_iter=300)\n",
    "    clusters = kmeans.fit_predict(text_features)\n",
    "    df[\"cluster\"] = clusters\n",
    "\n",
    "    cluster_names = name_clusters(df, \"cluster\", \"product_name_clean\")\n",
    "    df[\"cluster_name\"] = df[\"cluster\"].map(cluster_names)\n",
    "\n",
    "    cluster_results = compute_cluster_stats(df, \"cluster\")\n",
    "\n",
    "    mlflow.set_experiment(\"Product Clustering & Sentiment Analysis\")\n",
    "\n",
    "    with mlflow.start_run(run_name=\"kmeans_clustering_embedding\"):\n",
    "        mlflow.log_param(\"embedding_model\", embedding_model_name)\n",
    "        mlflow.log_param(\"cluster_method\", \"KMeans\")\n",
    "        mlflow.log_param(\"k_range\", list(k_range))\n",
    "        mlflow.log_param(\"optimal_k\", optimal_k)\n",
    "        mlflow.log_metric(\"silhouette_score\", float(np.max(silhouette_scores)))\n",
    "\n",
    "        for _, r in cluster_results.iterrows():\n",
    "            prefix = f\"cluster_{int(r['cluster_id'])}_{str(r['cluster_name']).replace(' ', '_')}\"\n",
    "            mlflow.log_metric(f\"{prefix}_count\", int(r[\"count\"]))\n",
    "            if not np.isnan(r[\"avg_sentiment\"]):\n",
    "                mlflow.log_metric(f\"{prefix}_avg_sentiment\", float(r[\"avg_sentiment\"]))\n",
    "                mlflow.log_metric(f\"{prefix}_positive_pct\", float(r[\"positive_pct\"]))\n",
    "            if \"avg_rating\" in r and not pd.isna(r[\"avg_rating\"]):\n",
    "                mlflow.log_metric(f\"{prefix}_avg_rating\", float(r[\"avg_rating\"]))\n",
    "            if \"high_rating_pct\" in r and not pd.isna(r[\"high_rating_pct\"]):\n",
    "                mlflow.log_metric(f\"{prefix}_high_rating_pct\", float(r[\"high_rating_pct\"]))\n",
    "            if \"recommend_pct\" in r and not pd.isna(r[\"recommend_pct\"]):\n",
    "                mlflow.log_metric(f\"{prefix}_recommend_pct\", float(r[\"recommend_pct\"]))\n",
    "\n",
    "        df.to_csv(\"clustered_products.csv\", index=False)\n",
    "        cluster_results.to_csv(\"cluster_stats.csv\", index=False)\n",
    "        mlflow.sklearn.log_model(kmeans, \"kmeans_model\")\n",
    "        mlflow.log_artifact(\"clustered_products.csv\")\n",
    "        mlflow.log_artifact(\"cluster_stats.csv\")\n",
    "\n",
    "    zero_shot_labels = [\n",
    "        \"Fire Tablet Special\",\n",
    "        \"AmazonBasics Performance Alkaline\",\n",
    "        \"Anon\",\n",
    "        \"Echo White Amazon\",\n",
    "        \"Fire Kids Edition\",\n",
    "        \"Fire Amazon With\",\n",
    "    ]\n",
    "\n",
    "    zs_preds, zs_scores = zero_shot_batch(\n",
    "        df[\"product_name_clean\"].tolist(),\n",
    "        labels=zero_shot_labels,\n",
    "        model_name=\"facebook/bart-large-mnli\",\n",
    "        batch_size=16,\n",
    "    )\n",
    "    df[\"zero_shot_label\"] = zs_preds\n",
    "    df[\"zero_shot_score\"] = zs_scores\n",
    "\n",
    "    cat_results = compute_category_stats(df, \"zero_shot_label\")\n",
    "\n",
    "    with mlflow.start_run(run_name=\"zero_shot_classification\", nested=True):\n",
    "        mlflow.log_param(\"zero_shot_model\", \"facebook/bart-large-mnli\")\n",
    "        mlflow.log_param(\"candidate_labels\", zero_shot_labels)\n",
    "\n",
    "        for _, r in cat_results.iterrows():\n",
    "            prefix = f\"zshot_{str(r['category']).replace(' ', '_')}\"\n",
    "            mlflow.log_metric(f\"{prefix}_count\", int(r[\"count\"]))\n",
    "            if not np.isnan(r[\"avg_sentiment\"]):\n",
    "                mlflow.log_metric(f\"{prefix}_avg_sentiment\", float(r[\"avg_sentiment\"]))\n",
    "                mlflow.log_metric(f\"{prefix}_positive_pct\", float(r[\"positive_pct\"]))\n",
    "            if \"avg_zero_shot_score\" in r and not pd.isna(r[\"avg_zero_shot_score\"]):\n",
    "                mlflow.log_metric(f\"{prefix}_avg_confidence\", float(r[\"avg_zero_shot_score\"]))\n",
    "            if \"avg_rating\" in r and not pd.isna(r[\"avg_rating\"]):\n",
    "                mlflow.log_metric(f\"{prefix}_avg_rating\", float(r[\"avg_rating\"]))\n",
    "            if \"high_rating_pct\" in r and not pd.isna(r[\"high_rating_pct\"]):\n",
    "                mlflow.log_metric(f\"{prefix}_high_rating_pct\", float(r[\"high_rating_pct\"]))\n",
    "            if \"recommend_pct\" in r and not pd.isna(r[\"recommend_pct\"]):\n",
    "                mlflow.log_metric(f\"{prefix}_recommend_pct\", float(r[\"recommend_pct\"]))\n",
    "\n",
    "        df.to_csv(\"zero_shot_products.csv\", index=False)\n",
    "        cat_results.to_csv(\"zero_shot_stats.csv\", index=False)\n",
    "        mlflow.log_artifact(\"zero_shot_products.csv\")\n",
    "        mlflow.log_artifact(\"zero_shot_stats.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
