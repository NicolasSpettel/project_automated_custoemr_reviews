{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a153b81",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Hugging Face Transformers\n",
    "\n",
    "This notebook demonstrates how to fine-tune several pre-trained transformer models for sentiment analysis on a custom dataset. It covers data loading, preprocessing, model training with class weights, and evaluation of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a7b80",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47014787",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "# NLTK Components\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Scikit-learn Tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# Suppress all warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download NLTK data for text processing\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6940f3fe",
   "metadata": {},
   "source": [
    "## Data Preparation and Preprocessing\n",
    "\n",
    "The `DataProcessor` class handles loading the raw data, cleaning the text, and creating a balanced dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418bd2f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    Handles data loading, cleaning, and preparation for model training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Initializes the DataProcessor with a dataset file path.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the CSV file containing the dataset.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "        self.df = self._load_data()\n",
    "\n",
    "    def _load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the CSV file into a DataFrame.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(self.filepath)\n",
    "        return df\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleans and tokenizes text by removing punctuation, numbers, and stop words.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned and tokenized text.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        text = str(text).lower()\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [\n",
    "            word for word in words if word not in self.stop_words and len(word) > 1\n",
    "        ]\n",
    "        return \" \".join(filtered_words)\n",
    "\n",
    "    def prepare_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Combines and cleans text, maps sentiment labels, and balances the dataset.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A balanced DataFrame ready for splitting and tokenization.\n",
    "        \"\"\"\n",
    "        self.df[\"full_review\"] = self.df.apply(\n",
    "            lambda row: f\"{str(row.get('title', ''))} {str(row.get('text', ''))}\".strip(),\n",
    "            axis=1,\n",
    "        )\n",
    "        self.df[\"cleaned_review\"] = self.df[\"full_review\"].apply(self._clean_text)\n",
    "        \n",
    "        self.df = self.df[\n",
    "            (self.df[\"cleaned_review\"].str.len() > 10) & (self.df[\"star_sentiment\"].notna())\n",
    "        ]\n",
    "        \n",
    "        sentiment_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "        self.df[\"label\"] = self.df[\"star_sentiment\"].map(sentiment_map)\n",
    "        \n",
    "        samples_per_class = {\"Negative\": 2400, \"Neutral\": 2800, \"Positive\": 3000}\n",
    "        balanced_dfs = []\n",
    "\n",
    "        for sentiment, class_id in sentiment_map.items():\n",
    "            class_df = self.df[self.df[\"label\"] == class_id]\n",
    "            n_samples = min(samples_per_class[sentiment], len(class_df))\n",
    "            balanced_dfs.append(class_df.sample(n=n_samples, random_state=42))\n",
    "\n",
    "        balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "        return balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ab5f3",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "The `ModelTrainer` class orchestrates the fine-tuning process. It handles dataset tokenization, creates a custom `Trainer` to use class weights for a imbalanced dataset, and evaluates the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b77cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Manages the training and evaluation of Hugging Face transformer models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame, eval_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes the trainer with training and evaluation data.\n",
    "\n",
    "        Args:\n",
    "            train_df (pd.DataFrame): DataFrame for training.\n",
    "            eval_df (pd.DataFrame): DataFrame for evaluation.\n",
    "        \"\"\"\n",
    "        self.train_df = train_df\n",
    "        self.eval_df = eval_df\n",
    "        self.results = {}\n",
    "        \n",
    "        train_labels = train_df[\"label\"].tolist()\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight=\"balanced\",\n",
    "            classes=np.unique(train_labels),\n",
    "            y=train_labels\n",
    "        )\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "    def _prepare_datasets(self, tokenizer) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"\n",
    "        Tokenizes and formats the data for the Hugging Face Trainer.\n",
    "        \"\"\"\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"cleaned_review\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=256\n",
    "            )\n",
    "\n",
    "        train_dataset = Dataset.from_pandas(self.train_df[[\"cleaned_review\", \"label\"]])\n",
    "        eval_dataset = Dataset.from_pandas(self.eval_df[[\"cleaned_review\", \"label\"]])\n",
    "\n",
    "        train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "        eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "        train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "        eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "    def _create_trainer(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataset,\n",
    "        eval_dataset,\n",
    "        training_args: TrainingArguments,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a custom Trainer class that uses a weighted cross-entropy loss function.\n",
    "        \"\"\"\n",
    "        class WeightedTrainer(Trainer):\n",
    "            def __init__(self, class_weights=None, **kwargs):\n",
    "                super().__init__(**kwargs)\n",
    "                self.class_weights = class_weights.to(self.args.device) if class_weights is not None else None\n",
    "\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.pop(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "                loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        return WeightedTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            class_weights=self.class_weights,\n",
    "        )\n",
    "\n",
    "    def train_and_evaluate_model(self, model_name: str, training_args: TrainingArguments) -> Dict:\n",
    "        \"\"\"\n",
    "        Trains and evaluates a single model, returning its performance metrics.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name,\n",
    "                num_labels=3,\n",
    "                id2label={0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"},\n",
    "                label2id={\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "            )\n",
    "\n",
    "            train_dataset, eval_dataset = self._prepare_datasets(tokenizer)\n",
    "\n",
    "            trainer = self._create_trainer(\n",
    "                model, tokenizer, train_dataset, eval_dataset, training_args\n",
    "            )\n",
    "\n",
    "            trainer.train()\n",
    "            \n",
    "            predictions = trainer.predict(eval_dataset)\n",
    "            pred_labels = predictions.predictions.argmax(axis=1)\n",
    "            true_labels = predictions.label_ids\n",
    "\n",
    "            f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "            report = classification_report(\n",
    "                true_labels,\n",
    "                pred_labels,\n",
    "                target_names=[\"Negative\", \"Neutral\", \"Positive\"],\n",
    "                output_dict=True\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"model_name\": model_name,\n",
    "                \"f1_score\": f1,\n",
    "                \"predictions\": pred_labels.tolist(),\n",
    "                \"true_labels\": true_labels.tolist(),\n",
    "                \"classification_report\": report\n",
    "            }\n",
    "            self.results[model_name] = result\n",
    "\n",
    "            del model, trainer, tokenizer\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"model_name\": model_name, \"error\": str(e)}\n",
    "\n",
    "    def run_all_models(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Trains and evaluates a predefined list of models and saves the results.\n",
    "        \"\"\"\n",
    "        models = [\n",
    "            \"distilbert-base-uncased\",\n",
    "            \"bert-base-uncased\",\n",
    "            \"roberta-base\",\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "        ]\n",
    "\n",
    "        default_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            logging_steps=500,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            report_to=\"none\",\n",
    "            disable_tqdm=False,\n",
    "            seed=42,\n",
    "            dataloader_pin_memory=True,\n",
    "            gradient_checkpointing=True,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            warmup_steps=50,\n",
    "        )\n",
    "\n",
    "        for model_name in models:\n",
    "            self.train_and_evaluate_model(model_name, default_args)\n",
    "\n",
    "        with open(\"model_results.json\", \"w\") as f:\n",
    "            serializable_results = {\n",
    "                model: {\n",
    "                    \"model_name\": result[\"model_name\"],\n",
    "                    \"f1_score\": float(result[\"f1_score\"]),\n",
    "                }\n",
    "                for model, result in self.results.items() if \"error\" not in result\n",
    "            }\n",
    "            json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "        with open(\"detailed_results.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.results, f)\n",
    "\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba268f69",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "This section initializes the data processing and model training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15284027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "FILE_PATH = \"Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_cleaned.csv\"\n",
    "processor = DataProcessor(FILE_PATH)\n",
    "processed_df = processor.prepare_data()\n",
    "\n",
    "# Split data into training and evaluation sets\n",
    "train_df, eval_df = train_test_split(\n",
    "    processed_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=processed_df[\"label\"],\n",
    ")\n",
    "\n",
    "# Initialize and run the model training pipeline\n",
    "trainer = ModelTrainer(train_df, eval_df)\n",
    "all_results = trainer.run_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2dbab",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "A summary of the performance for each model is displayed here. Detailed results are saved in `model_results.json` and `detailed_results.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85abb9e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Display summary of all model results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY OF ALL MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<45} {'F1 Score':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, result in all_results.items():\n",
    "    if \"error\" not in result:\n",
    "        print(f\"{model_name:<45} {result['f1_score']:<10.4f}\")\n",
    "    else:\n",
    "        print(f\"{model_name:<45} {'ERROR':<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9843d96",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Visualization Class\n",
    "class ModelVisualizer:\n",
    "    \"\"\"\n",
    "    Generates various plots and analyses to compare the performance\n",
    "    of the trained models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, results_json_path: str = 'model_results.json', \n",
    "                 detailed_results_pkl_path: str = 'detailed_results.pkl'):\n",
    "        \"\"\"\n",
    "        Initializes the visualizer by loading model results.\n",
    "\n",
    "        Args:\n",
    "            results_json_path (str): Path to the JSON file with summary results.\n",
    "            detailed_results_pkl_path (str): Path to the pickle file with detailed results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(results_json_path, 'r') as f:\n",
    "                self.results = json.load(f)\n",
    "            \n",
    "            with open(detailed_results_pkl_path, 'rb') as f:\n",
    "                self.detailed_results = pickle.load(f)\n",
    "\n",
    "            self.model_names = [name for name in self.results.keys() if \"error\" not in self.results.get(name, {})]\n",
    "            self.label_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Required results files not found. \"\n",
    "                f\"Please ensure the training script ran successfully and generated '{e.filename}'.\"\n",
    "            ) from e\n",
    "\n",
    "    def _clean_model_name(self, name: str) -> str:\n",
    "        \"\"\"Helper to clean up model names for plot titles.\"\"\"\n",
    "        clean_name = name.replace('cardiffnlp/', '').replace('nlptown/', '')\n",
    "        clean_name = clean_name.replace('-base-uncased', '').replace('-base', '')\n",
    "        clean_name = clean_name.replace('twitter-roberta', 'Twitter RoBERTa')\n",
    "        return clean_name.title()\n",
    "\n",
    "    def create_confusion_matrices(self) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Generates and displays normalized confusion matrices for all models.\n",
    "        \"\"\"\n",
    "        n_models = len(self.model_names)\n",
    "        if n_models == 0:\n",
    "            return\n",
    "\n",
    "        cols = 3\n",
    "        rows = (n_models + cols - 1) // cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "        axes = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "        for idx, model in enumerate(self.model_names):\n",
    "            if idx >= len(axes):\n",
    "                break\n",
    "            \n",
    "            true_labels = self.detailed_results[model]['true_labels']\n",
    "            pred_labels = self.detailed_results[model]['predictions']\n",
    "            \n",
    "            cm = confusion_matrix(true_labels, pred_labels)\n",
    "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "            ax = axes[idx]\n",
    "            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                        xticklabels=self.label_names, yticklabels=self.label_names,\n",
    "                        ax=ax, cbar_kws={'shrink': 0.8})\n",
    "            \n",
    "            clean_name = self._clean_model_name(model)\n",
    "            f1_score_val = self.detailed_results[model]['f1_score']\n",
    "            ax.set_title(f'{clean_name}\\nF1: {f1_score_val:.3f}', fontweight='bold')\n",
    "            ax.set_ylabel('True Label')\n",
    "            ax.set_xlabel('Predicted Label')\n",
    "\n",
    "        for idx in range(n_models, len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "            \n",
    "        fig.suptitle('Normalized Confusion Matrices', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def create_class_performance_comparison(self) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Creates heatmaps comparing precision, recall, and F1-score across models.\n",
    "        \"\"\"\n",
    "        metrics_data = []\n",
    "        for model in self.model_names:\n",
    "            report = self.detailed_results[model]['classification_report']\n",
    "            for class_name in self.label_names:\n",
    "                if class_name in report:\n",
    "                    metrics_data.append({\n",
    "                        'Model': self._clean_model_name(model),\n",
    "                        'Class': class_name,\n",
    "                        'Precision': report[class_name]['precision'],\n",
    "                        'Recall': report[class_name]['recall'],\n",
    "                        'F1-Score': report[class_name]['f1-score']\n",
    "                    })\n",
    "        \n",
    "        df = pd.DataFrame(metrics_data)\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "        for idx, metric in enumerate(metrics):\n",
    "            ax = axes[idx]\n",
    "            pivot_df = df.pivot(index='Model', columns='Class', values=metric)\n",
    "            sns.heatmap(pivot_df, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=ax, \n",
    "                        cbar_kws={'shrink': 0.8}, vmin=0.5, vmax=1.0)\n",
    "            ax.set_title(f'{metric} by Class', fontweight='bold', fontsize=14)\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel('Model' if idx == 0 else '')\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "        fig.suptitle('Per-Class Performance Metrics Across Models', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def create_error_analysis(self) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Analyzes and visualizes error rates and overall accuracy.\n",
    "        \"\"\"\n",
    "        error_data, accuracies = [], []\n",
    "        \n",
    "        for model in self.model_names:\n",
    "            true_labels = np.array(self.detailed_results[model]['true_labels'])\n",
    "            pred_labels = np.array(self.detailed_results[model]['predictions'])\n",
    "            \n",
    "            accuracy = (pred_labels == true_labels).mean()\n",
    "            accuracies.append((self._clean_model_name(model), accuracy))\n",
    "            \n",
    "            for class_idx, class_name in enumerate(self.label_names):\n",
    "                class_mask = (true_labels == class_idx)\n",
    "                if np.any(class_mask):\n",
    "                    error_rate = (pred_labels[class_mask] != class_idx).mean()\n",
    "                    error_data.append({\n",
    "                        'Model': self._clean_model_name(model),\n",
    "                        'Class': class_name,\n",
    "                        'Error_Rate': error_rate\n",
    "                    })\n",
    "\n",
    "        df_errors = pd.DataFrame(error_data)\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        pivot_errors = df_errors.pivot(index='Model', columns='Class', values='Error_Rate')\n",
    "        sns.heatmap(pivot_errors, annot=True, fmt='.3f', cmap='Reds', ax=ax1, cbar_kws={'shrink': 0.8})\n",
    "        ax1.set_title('Error Rates by Class', fontweight='bold', fontsize=14)\n",
    "        ax1.set_ylabel('Model')\n",
    "\n",
    "        model_names_clean, accuracy_vals = zip(*accuracies)\n",
    "        sns.barplot(x=list(accuracy_vals), y=list(model_names_clean), palette='viridis', ax=ax2)\n",
    "        ax2.set_title('Overall Accuracy Comparison', fontweight='bold', fontsize=14)\n",
    "        ax2.set_xlabel('Accuracy')\n",
    "        ax2.set_xlim(0, 1.0)\n",
    "        ax2.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        fig.suptitle('Error Analysis Across Models', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def create_model_ranking_summary(self) -> Tuple[plt.Figure, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Generates a summary table and radar chart for the top models.\n",
    "        \"\"\"\n",
    "        summary_data = []\n",
    "        for model in self.model_names:\n",
    "            true_labels = np.array(self.detailed_results[model]['true_labels'])\n",
    "            pred_labels = np.array(self.detailed_results[model]['predictions'])\n",
    "            report = self.detailed_results[model]['classification_report']\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Model': self._clean_model_name(model),\n",
    "                'Accuracy': (pred_labels == true_labels).mean(),\n",
    "                'Weighted F1': self.detailed_results[model]['f1_score'],\n",
    "                'Negative F1': report.get('Negative', {}).get('f1-score', 0),\n",
    "                'Neutral F1': report.get('Neutral', {}).get('f1-score', 0),\n",
    "                'Positive F1': report.get('Positive', {}).get('f1-score', 0),\n",
    "            })\n",
    "        \n",
    "        df_summary = pd.DataFrame(summary_data).sort_values('Weighted F1', ascending=False)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        ax1.axis('tight')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        table = ax1.table(cellText=df_summary.round(3).values, colLabels=df_summary.columns,\n",
    "                          cellLoc='center', loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1.2, 2)\n",
    "        ax1.set_title('Model Performance Ranking\\n(Sorted by Weighted F1)', fontweight='bold', fontsize=14, pad=20)\n",
    "\n",
    "        # Radar chart for top 3 models\n",
    "        top_3 = df_summary.head(3)\n",
    "        categories = ['Accuracy', 'Weighted F1', 'Negative F1', 'Neutral F1', 'Positive F1']\n",
    "        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        ax2 = plt.subplot(122, polar=True)\n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "        for idx, (_, row) in enumerate(top_3.iterrows()):\n",
    "            values = [row[cat] for cat in categories] + [row[categories[0]]]\n",
    "            ax2.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[idx], alpha=0.8)\n",
    "            ax2.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "            \n",
    "        ax2.set_xticks(angles[:-1])\n",
    "        ax2.set_xticklabels(categories)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.set_title('Top 3 Models - Performance Radar', fontweight='bold', fontsize=14, pad=30)\n",
    "        ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig, df_summary\n",
    "\n",
    "    def generate_all_visualizations(self):\n",
    "        \"\"\"Generates and displays all analysis plots.\"\"\"\n",
    "        print(\"Generating Model Comparison Visualizations...\")\n",
    "        self.create_confusion_matrices()\n",
    "        self.create_class_performance_comparison()\n",
    "        self.create_error_analysis()\n",
    "        fig, summary_df = self.create_model_ranking_summary()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"FINAL RECOMMENDATIONS\")\n",
    "        print(\"=\" * 60)\n",
    "        best_model = summary_df.iloc[0]['Model']\n",
    "        best_f1 = summary_df.iloc[0]['Weighted F1']\n",
    "        print(f\"Best Overall Model: {best_model}\")\n",
    "        print(f\"Best F1 Score: {best_f1:.4f}\")\n",
    "        print(\"\\n Key Insights:\")\n",
    "        print(f\"• {best_model} shows the best overall performance\")\n",
    "        print(f\"• Accuracy range: {summary_df['Accuracy'].min():.3f} - {summary_df['Accuracy'].max():.3f}\")\n",
    "        print(f\"• F1 range: {summary_df['Weighted F1'].min():.3f} - {summary_df['Weighted F1'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ca693",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning and Single Model Analysis\n",
    "\n",
    "This section demonstrates how to fine-tune a single model with custom hyperparameters and then visualizes its individual performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dde9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom hyperparameters for RoBERTa\n",
    "roberta_hyperparameters = TrainingArguments(\n",
    "    output_dir=\"./roberta_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=500,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=True,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "# Re-run a specific model with the new hyperparameters\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RE-RUNNING ROBERTA MODEL WITH EXTERNAL HYPERPARAMETERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# The 'trainer' object must be available from the previous cell's execution.\n",
    "# This part assumes a `ModelTrainer` instance `trainer` exists.\n",
    "try:\n",
    "    roberta_results = trainer.train_and_evaluate_model(\"roberta-base\", roberta_hyperparameters)\n",
    "except NameError:\n",
    "    print(\"Error: `trainer` object not found. Please run the Model Training cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd34dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single model visualization\n",
    "try:\n",
    "    # Load the results from the saved files\n",
    "    with open('detailed_results.pkl', 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "\n",
    "    model_name = \"roberta-base\"\n",
    "    label_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "    \n",
    "    if model_name in all_results and \"error\" not in all_results[model_name]:\n",
    "        print(f\"\\n Generating single-model visualization for {model_name}...\")\n",
    "        \n",
    "        true_labels = np.array(all_results[model_name]['true_labels'])\n",
    "        pred_labels = np.array(all_results[model_name]['predictions'])\n",
    "        \n",
    "        # --- Confusion Matrix Visualization ---\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        cm_normalized = confusion_matrix(true_labels, pred_labels, normalize='true')\n",
    "        \n",
    "        sns.heatmap(\n",
    "            cm_normalized,\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            cmap='Blues',\n",
    "            xticklabels=label_names,\n",
    "            yticklabels=label_names,\n",
    "            ax=ax\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f'Confusion Matrix for {model_name} (Normalized)', fontweight='bold')\n",
    "        ax.set_ylabel('True Label')\n",
    "        ax.set_xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # --- Error Rate Analysis ---\n",
    "        print(\"\\n📈 Error Analysis:\")\n",
    "        accuracy = (pred_labels == true_labels).mean()\n",
    "        print(f\"• Overall Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        per_class_errors = {}\n",
    "        for class_idx, class_name in enumerate(label_names):\n",
    "            class_mask = (true_labels == class_idx)\n",
    "            if np.any(class_mask):\n",
    "                error_rate = (pred_labels[class_mask] != class_idx).mean()\n",
    "                per_class_errors[class_name] = error_rate\n",
    "        \n",
    "        print(\"• Per-Class Error Rates:\")\n",
    "        for class_name, rate in per_class_errors.items():\n",
    "            print(f\"  - {class_name}: {rate:.4f}\")\n",
    "    else:\n",
    "        print(f\"Error: Results for model '{model_name}' not found or contain errors. Please ensure the model was trained successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find 'detailed_results.pkl'. Please ensure the model training and evaluation step was completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "project_nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
