{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2559a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import warnings\n",
    "\n",
    "# Filter out warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set global plot style\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Handles model training and evaluation.\"\"\"\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame, eval_df: pd.DataFrame):\n",
    "        self.train_df = train_df\n",
    "        self.eval_df = eval_df\n",
    "        self.results = {}\n",
    "\n",
    "        train_labels = train_df[\"label\"].tolist()\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight=\"balanced\",\n",
    "            classes=np.unique(train_labels),\n",
    "            y=train_labels,\n",
    "        )\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "    def _prepare_datasets(self, tokenizer) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"Convert dataframes to tokenized datasets.\"\"\"\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"cleaned_review\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "            )\n",
    "\n",
    "        train_dataset = Dataset.from_pandas(self.train_df[[\"cleaned_review\", \"label\"]])\n",
    "        eval_dataset = Dataset.from_pandas(self.eval_df[[\"cleaned_review\", \"label\"]])\n",
    "\n",
    "        train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "        eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "        train_dataset.set_format(\n",
    "            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    "        )\n",
    "        eval_dataset.set_format(\n",
    "            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    "        )\n",
    "\n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "    def _create_trainer(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataset,\n",
    "        eval_dataset,\n",
    "        training_args: TrainingArguments,\n",
    "    ):\n",
    "        \"\"\"Create custom trainer with class weights.\"\"\"\n",
    "\n",
    "        class WeightedTrainer(Trainer):\n",
    "            def __init__(self, class_weights=None, **kwargs):\n",
    "                super().__init__(**kwargs)\n",
    "                self.class_weights = (\n",
    "                    class_weights.to(self.args.device) if class_weights is not None else None\n",
    "                )\n",
    "\n",
    "            def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get(\"logits\")\n",
    "\n",
    "                if self.class_weights is not None:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "                else:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "                loss = loss_fct(\n",
    "                    logits.view(-1, model.config.num_labels), labels.view(-1)\n",
    "                )\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        return WeightedTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            class_weights=self.class_weights,\n",
    "        )\n",
    "\n",
    "    def train_and_evaluate_model(self, model_name: str, training_args: TrainingArguments) -> Dict:\n",
    "        \"\"\"Train and evaluate a single model.\"\"\"\n",
    "        print(f\"\\n{'='*20} {model_name} {'='*20}\")\n",
    "\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name,\n",
    "                num_labels=3,\n",
    "                id2label={0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"},\n",
    "                label2id={\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2},\n",
    "            )\n",
    "\n",
    "            train_dataset, eval_dataset = self._prepare_datasets(tokenizer)\n",
    "\n",
    "            trainer = self._create_trainer(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                training_args,\n",
    "            )\n",
    "\n",
    "            trainer.train()\n",
    "\n",
    "            # Save the trained model and tokenizer\n",
    "            save_directory = f\"./{model_name.replace('/', '_')}-finetuned\"\n",
    "            print(f\"Saving model and tokenizer to {save_directory}\")\n",
    "            trainer.save_model(save_directory)\n",
    "\n",
    "            predictions = trainer.predict(eval_dataset)\n",
    "            pred_labels = predictions.predictions.argmax(axis=1)\n",
    "            true_labels = predictions.label_ids\n",
    "\n",
    "            f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "            label_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "            report = classification_report(\n",
    "                true_labels, pred_labels, target_names=label_names, output_dict=True\n",
    "            )\n",
    "\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(f\"{'Class':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "            print(\"-\" * 45)\n",
    "            for label in label_names:\n",
    "                metrics = report[label]\n",
    "                print(\n",
    "                    f\"{label:<10} {metrics['precision']:<10.3f} \"\n",
    "                    f\"{metrics['recall']:<10.3f} {metrics['f1-score']:<10.3f}\"\n",
    "                )\n",
    "\n",
    "            result = {\n",
    "                \"model_name\": model_name,\n",
    "                \"f1_score\": f1,\n",
    "                \"predictions\": pred_labels.tolist(),\n",
    "                \"true_labels\": true_labels.tolist(),\n",
    "                \"classification_report\": report,\n",
    "            }\n",
    "\n",
    "            self.results[model_name] = result\n",
    "\n",
    "            del model, trainer, tokenizer\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model_name}: {str(e)}\")\n",
    "            return {\"model_name\": model_name, \"error\": str(e)}\n",
    "\n",
    "    def run_all_models(self) -> Dict:\n",
    "        \"\"\"Train and evaluate all models.\"\"\"\n",
    "        models = [\n",
    "            \"distilbert-base-uncased\",\n",
    "            \"bert-base-uncased\",\n",
    "            \"roberta-base\",\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "        ]\n",
    "\n",
    "        # Define a default set of hyperparameters\n",
    "        default_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            logging_steps=500,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            report_to=\"none\",\n",
    "            disable_tqdm=False,\n",
    "            seed=42,\n",
    "            dataloader_pin_memory=True,\n",
    "            gradient_checkpointing=True,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            warmup_steps=50,\n",
    "        )\n",
    "\n",
    "        for model_name in models:\n",
    "            self.train_and_evaluate_model(model_name, default_args)\n",
    "\n",
    "        with open(\"model_results.json\", \"w\") as f:\n",
    "            serializable_results = {}\n",
    "            for model, result in self.results.items():\n",
    "                if \"error\" not in result:\n",
    "                    serializable_results[model] = {\n",
    "                        \"model_name\": result[\"model_name\"],\n",
    "                        \"f1_score\": float(result[\"f1_score\"]),\n",
    "                        \"predictions\": result[\"predictions\"],\n",
    "                        \"true_labels\": result[\"true_labels\"],\n",
    "                    }\n",
    "\n",
    "        with open(\"model_results.json\", \"w\") as f:\n",
    "            json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "        with open(\"detailed_results.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.results, f)\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"SUMMARY OF ALL MODELS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"{'Model':<45} {'F1 Score':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for model_name, result in self.results.items():\n",
    "            if \"error\" not in result:\n",
    "                print(f\"{model_name:<45} {result['f1_score']:<10.4f}\")\n",
    "            else:\n",
    "                print(f\"{model_name:<45} {'ERROR':<10}\")\n",
    "\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models\n",
    "trainer = ModelTrainer(train_df, eval_df)\n",
    "all_results = trainer.run_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVisualizer:\n",
    "    \"\"\"Create comprehensive visualizations comparing sentiment analysis models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, results_file=\"model_results.json\", detailed_file=\"detailed_results.pkl\"\n",
    "    ):\n",
    "        \"\"\"Load model results and prepare for visualization.\"\"\"\n",
    "        with open(results_file, \"r\") as f:\n",
    "            self.results = json.load(f)\n",
    "\n",
    "        with open(detailed_file, \"rb\") as f:\n",
    "            self.detailed_results = pickle.load(f)\n",
    "\n",
    "        self.label_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "        self.model_names = list(self.results.keys())\n",
    "\n",
    "        print(f\"Loaded results for {len(self.model_names)} models\")\n",
    "\n",
    "    def _clean_model_name(self, name: str) -> str:\n",
    "        \"\"\"Helper to clean model names for display.\"\"\"\n",
    "        clean_name = name.replace(\"cardiffnlp/\", \"\").replace(\"nlptown/\", \"\")\n",
    "        clean_name = clean_name.replace(\"-base-uncased\", \"\").replace(\"-base\", \"\")\n",
    "        clean_name = clean_name.replace(\"twitter-roberta\", \"Twitter RoBERTa\")\n",
    "        clean_name = clean_name.replace(\"bert-multilingual\", \"Multilingual BERT\")\n",
    "        clean_name = clean_name.title().replace(\"Distilbert\", \"DistilBERT\").replace(\"Roberta\", \"RoBERTa\")\n",
    "        return clean_name\n",
    "\n",
    "    def create_f1_comparison(self):\n",
    "        \"\"\"Create F1 score comparison bar chart.\"\"\"\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "        models = [self._clean_model_name(m) for m in self.model_names]\n",
    "        f1_scores = [self.results[m][\"f1_score\"] for m in self.model_names]\n",
    "\n",
    "        colors = sns.color_palette(\"viridis\", len(models))\n",
    "        bars = ax.bar(\n",
    "            models, f1_scores, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=1\n",
    "        )\n",
    "\n",
    "        for bar, score in zip(bars, f1_scores):\n",
    "            height = bar.get_height()\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height + 0.005,\n",
    "                f\"{score:.3f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "        ax.set_title(\n",
    "            \"Model Performance Comparison\\nWeighted F1 Scores\",\n",
    "            fontsize=16,\n",
    "            fontweight=\"bold\",\n",
    "            pad=20,\n",
    "        )\n",
    "        ax.set_ylabel(\"Weighted F1 Score\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Model\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_ylim(0, max(f1_scores) * 1.15)\n",
    "\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.grid(axis=\"y\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def create_confusion_matrices(self):\n",
    "        \"\"\"Create confusion matrices for all models.\"\"\"\n",
    "        n_models = len(self.model_names)\n",
    "        cols = 3\n",
    "        rows = (n_models + cols - 1) // cols\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "        axes = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "        for idx, model in enumerate(self.model_names):\n",
    "            if idx >= len(axes):\n",
    "                break\n",
    "\n",
    "            true_labels = self.results[model][\"true_labels\"]\n",
    "            pred_labels = self.results[model][\"predictions\"]\n",
    "\n",
    "            cm = confusion_matrix(true_labels, pred_labels)\n",
    "            cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "            ax = axes[idx]\n",
    "            sns.heatmap(\n",
    "                cm_normalized,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                cmap=\"Blues\",\n",
    "                xticklabels=self.label_names,\n",
    "                yticklabels=self.label_names,\n",
    "                ax=ax,\n",
    "                cbar_kws={\"shrink\": 0.8},\n",
    "            )\n",
    "\n",
    "            clean_name = self._clean_model_name(model)\n",
    "\n",
    "            ax.set_title(\n",
    "                f'{clean_name}\\nF1: {self.results[model][\"f1_score\"]:.3f}',\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "            ax.set_ylabel(\"True Label\")\n",
    "            ax.set_xlabel(\"Predicted Label\")\n",
    "\n",
    "        for idx in range(n_models, len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "\n",
    "        fig.suptitle(\"Confusion Matrices (Normalized)\", fontsize=16, fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def create_class_performance_comparison(self):\n",
    "        \"\"\"Create detailed class-wise performance comparison.\"\"\"\n",
    "        metrics_data = []\n",
    "\n",
    "        for model in self.model_names:\n",
    "            if (\n",
    "                model in self.detailed_results\n",
    "                and \"classification_report\" in self.detailed_results[model]\n",
    "            ):\n",
    "                report = self.detailed_results[model][\"classification_report\"]\n",
    "\n",
    "                for class_name in self.label_names:\n",
    "                    if class_name in report:\n",
    "                        metrics_data.append(\n",
    "                            {\n",
    "                                \"Model\": self._clean_model_name(model),\n",
    "                                \"Class\": class_name,\n",
    "                                \"Precision\": report[class_name][\"precision\"],\n",
    "                                \"Recall\": report[class_name][\"recall\"],\n",
    "                                \"F1-Score\": report[class_name][\"f1-score\"],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        df = pd.DataFrame(metrics_data)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "        metrics = [\"Precision\", \"Recall\", \"F1-Score\"]\n",
    "\n",
    "        for idx, metric in enumerate(metrics):\n",
    "            ax = axes[idx]\n",
    "            pivot_df = df.pivot(index=\"Model\", columns=\"Class\", values=metric)\n",
    "\n",
    "            sns.heatmap(\n",
    "                pivot_df,\n",
    "                annot=True,\n",
    "                fmt=\".3f\",\n",
    "                cmap=\"RdYlBu_r\",\n",
    "                ax=ax,\n",
    "                cbar_kws={\"shrink\": 0.8},\n",
    "                vmin=0.5,\n",
    "                vmax=1.0,\n",
    "            )\n",
    "\n",
    "            ax.set_title(f\"{metric} by Class\", fontweight=\"bold\", fontsize=14)\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"Model\" if idx == 0 else \"\")\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "        fig.suptitle(\n",
    "            \"Per-Class Performance Metrics Across Models\",\n",
    "            fontsize=16,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def create_error_analysis(self):\n",
    "        \"\"\"Analyze prediction errors across models.\"\"\"\n",
    "        error_data = []\n",
    "\n",
    "        for model in self.model_names:\n",
    "            true_labels = np.array(self.results[model][\"true_labels\"])\n",
    "            pred_labels = np.array(self.results[model][\"predictions\"])\n",
    "\n",
    "            for class_idx, class_name in enumerate(self.label_names):\n",
    "                class_mask = true_labels == class_idx\n",
    "                class_true = true_labels[class_mask]\n",
    "                class_predictions = pred_labels[class_mask]\n",
    "\n",
    "                if len(class_true) > 0:\n",
    "                    error_rate = (class_predictions != class_true).mean()\n",
    "                    error_data.append(\n",
    "                        {\n",
    "                            \"Model\": self._clean_model_name(model),\n",
    "                            \"Class\": class_name,\n",
    "                            \"Error_Rate\": error_rate,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        df_errors = pd.DataFrame(error_data)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "        pivot_errors = df_errors.pivot(index=\"Model\", columns=\"Class\", values=\"Error_Rate\")\n",
    "        sns.heatmap(\n",
    "            pivot_errors,\n",
    "            annot=True,\n",
    "            fmt=\".3f\",\n",
    "            cmap=\"Reds\",\n",
    "            ax=ax1,\n",
    "            cbar_kws={\"shrink\": 0.8},\n",
    "        )\n",
    "        ax1.set_title(\"Error Rates by Class\", fontweight=\"bold\", fontsize=14)\n",
    "        ax1.set_ylabel(\"Model\")\n",
    "\n",
    "        accuracies = []\n",
    "        model_names_clean = []\n",
    "\n",
    "        for model in self.model_names:\n",
    "            true_labels = np.array(self.results[model][\"true_labels\"])\n",
    "            pred_labels = np.array(self.results[model][\"predictions\"])\n",
    "            accuracy = (pred_labels == true_labels).mean()\n",
    "            accuracies.append(accuracy)\n",
    "            model_names_clean.append(self._clean_model_name(model))\n",
    "\n",
    "        colors = sns.color_palette(\"plasma\", len(model_names_clean))\n",
    "        bars = ax2.barh(model_names_clean, accuracies, color=colors, alpha=0.8)\n",
    "\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            width = bar.get_width()\n",
    "            ax2.text(\n",
    "                width + 0.005,\n",
    "                bar.get_y() + bar.get_height() / 2.0,\n",
    "                f\"{acc:.3f}\",\n",
    "                ha=\"left\",\n",
    "                va=\"center\",\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "        ax2.set_title(\"Overall Accuracy Comparison\", fontweight=\"bold\", fontsize=14)\n",
    "        ax2.set_xlabel(\"Accuracy\")\n",
    "        ax2.set_xlim(0, max(accuracies) * 1.1)\n",
    "        ax2.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "        fig.suptitle(\"Error Analysis Across Models\", fontsize=16, fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def create_model_ranking_summary(self):\n",
    "        \"\"\"Create a comprehensive ranking summary.\"\"\"\n",
    "        summary_data = []\n",
    "\n",
    "        for model in self.model_names:\n",
    "            true_labels = np.array(self.results[model][\"true_labels\"])\n",
    "            pred_labels = np.array(self.results[model][\"predictions\"])\n",
    "\n",
    "            accuracy = (pred_labels == true_labels).mean()\n",
    "            f1_score = self.results[model][\"f1_score\"]\n",
    "\n",
    "            if (\n",
    "                model in self.detailed_results\n",
    "                and \"classification_report\" in self.detailed_results[model]\n",
    "            ):\n",
    "                report = self.detailed_results[model][\"classification_report\"]\n",
    "                neg_f1 = report.get(\"Negative\", {}).get(\"f1-score\", 0)\n",
    "                neu_f1 = report.get(\"Neutral\", {}).get(\"f1-score\", 0)\n",
    "                pos_f1 = report.get(\"Positive\", {}).get(\"f1-score\", 0)\n",
    "            else:\n",
    "                neg_f1 = neu_f1 = pos_f1 = 0\n",
    "\n",
    "            clean_name = self._clean_model_name(model)\n",
    "\n",
    "            summary_data.append(\n",
    "                {\n",
    "                    \"Model\": clean_name,\n",
    "                    \"Accuracy\": accuracy,\n",
    "                    \"Weighted F1\": f1_score,\n",
    "                    \"Negative F1\": neg_f1,\n",
    "                    \"Neutral F1\": neu_f1,\n",
    "                    \"Positive F1\": pos_f1,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        df_summary = pd.DataFrame(summary_data)\n",
    "        df_summary = df_summary.sort_values(\"Weighted F1\", ascending=False)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "        ax1.axis(\"tight\")\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "        table_data = df_summary.round(3)\n",
    "        table = ax1.table(\n",
    "            cellText=table_data.values,\n",
    "            colLabels=table_data.columns,\n",
    "            cellLoc=\"center\",\n",
    "            loc=\"center\",\n",
    "        )\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1.2, 2)\n",
    "\n",
    "        for i in range(len(table_data.columns)):\n",
    "            table[(0, i)].set_facecolor(\"#40466e\")\n",
    "            table[(0, i)].set_text_props(weight=\"bold\", color=\"white\")\n",
    "\n",
    "        for i in range(1, len(table_data) + 1):\n",
    "            if i == 1:\n",
    "                for j in range(len(table_data.columns)):\n",
    "                    table[(i, j)].set_facecolor(\"#d4edda\")\n",
    "            elif i == len(table_data):\n",
    "                for j in range(len(table_data.columns)):\n",
    "                    table[(i, j)].set_facecolor(\"#f8d7da\")\n",
    "\n",
    "        ax1.set_title(\n",
    "            \"Model Performance Ranking\\n(Sorted by Weighted F1)\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            pad=20,\n",
    "        )\n",
    "\n",
    "        top_3_models = df_summary.head(3)\n",
    "        categories = [\n",
    "            \"Accuracy\",\n",
    "            \"Weighted F1\",\n",
    "            \"Negative F1\",\n",
    "            \"Neutral F1\",\n",
    "            \"Positive F1\",\n",
    "        ]\n",
    "\n",
    "        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "        angles += angles[:1]\n",
    "\n",
    "        ax2 = plt.subplot(122, projection=\"polar\")\n",
    "        colors = [\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"]\n",
    "\n",
    "        for idx, (_, row) in enumerate(top_3_models.iterrows()):\n",
    "            values = [\n",
    "                row[\"Accuracy\"],\n",
    "                row[\"Weighted F1\"],\n",
    "                row[\"Negative F1\"],\n",
    "                row[\"Neutral F1\"],\n",
    "                row[\"Positive F1\"],\n",
    "            ]\n",
    "            values += values[:1]\n",
    "\n",
    "            ax2.plot(\n",
    "                angles,\n",
    "                values,\n",
    "                \"o-\",\n",
    "                linewidth=2,\n",
    "                label=row[\"Model\"],\n",
    "                color=colors[idx],\n",
    "                alpha=0.8,\n",
    "            )\n",
    "            ax2.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "        ax2.set_xticks(angles[:-1])\n",
    "        ax2.set_xticklabels(categories)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.set_title(\n",
    "            \"Top 3 Models - Performance Radar\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            pad=30,\n",
    "        )\n",
    "        ax2.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.0))\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return fig, df_summary\n",
    "\n",
    "    def generate_all_visualizations(self):\n",
    "        \"\"\"Generate all visualizations.\"\"\"\n",
    "        print(\"🎨 Generating Model Comparison Visualizations...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        print(\"\\n1. F1 Score Comparison\")\n",
    "        self.create_f1_comparison()\n",
    "\n",
    "        print(\"\\n2. Confusion Matrices\")\n",
    "        self.create_confusion_matrices()\n",
    "\n",
    "        print(\"\\n3. Class-wise Performance\")\n",
    "        self.create_class_performance_comparison()\n",
    "\n",
    "        print(\"\\n4. Error Analysis\")\n",
    "        self.create_error_analysis()\n",
    "\n",
    "        print(\"\\n5. Model Ranking Summary\")\n",
    "        fig, summary_df = self.create_model_ranking_summary()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"📊 FINAL RECOMMENDATIONS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        best_model = summary_df.iloc[0][\"Model\"]\n",
    "        best_f1 = summary_df.iloc[0][\"Weighted F1\"]\n",
    "\n",
    "        print(f\"🏆 Best Overall Model: {best_model}\")\n",
    "        print(f\"📈 Best F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "        print(\"\\n📝 Key Insights:\")\n",
    "        print(f\"• {summary_df.iloc[0]['Model']} shows the best overall performance\")\n",
    "        print(\n",
    "            f\"• Accuracy range: {summary_df['Accuracy'].min():.3f} - {summary_df['Accuracy'].max():.3f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"• F1 range: {summary_df['Weighted F1'].min():.3f} - {summary_df['Weighted F1'].max():.3f}\"\n",
    "        )\n",
    "\n",
    "        if summary_df[\"Neutral F1\"].min() < 0.7:\n",
    "            print(\"• Neutral class appears challenging for all models\")\n",
    "\n",
    "        return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ecc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the visualizations\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        visualizer = ModelVisualizer()\n",
    "        summary = visualizer.generate_all_visualizations()\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            \"Error: Could not find results files. Make sure model training completed successfully.\"\n",
    "        )\n",
    "        print(\"Looking for: model_results.json and detailed_results.pkl\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visualizations: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e84a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_hyperparameters = TrainingArguments(\n",
    "    output_dir=\"./roberta_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=500,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=True,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "# Train the RoBERTa model using the custom hyperparameters\n",
    "print(\"RE-RUNNING ROBERTA MODEL WITH EXTERNAL HYPERPARAMETERS\")\n",
    "roberta_results = trainer.train_and_evaluate_model(\"roberta-base\", roberta_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb50b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model to visualize\n",
    "model_name = \"roberta-base\"\n",
    "label_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "try:\n",
    "    # Load the results from the saved files\n",
    "    with open(\"model_results.json\", \"r\") as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    with open(\"detailed_results.pkl\", \"rb\") as f:\n",
    "        detailed_results = pickle.load(f)\n",
    "\n",
    "    if model_name in results and model_name in detailed_results:\n",
    "        print(f\"\\n Generating visualizations for the {model_name} model...\")\n",
    "\n",
    "        # --- Confusion Matrix Visualization ---\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "        true_labels = np.array(results[model_name][\"true_labels\"])\n",
    "        pred_labels = np.array(results[model_name][\"predictions\"])\n",
    "\n",
    "        # Create normalized confusion matrix\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # Plot heatmap\n",
    "        sns.heatmap(\n",
    "            cm_normalized,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=label_names,\n",
    "            yticklabels=label_names,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        ax.set_title(\n",
    "            \"Confusion Matrix for RoBERTa-base (Normalized)\", fontweight=\"bold\"\n",
    "        )\n",
    "        ax.set_ylabel(\"True Label\")\n",
    "        ax.set_xlabel(\"Predicted Label\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # --- Error Rate Calculation and Display ---\n",
    "        print(\"\\n📈 Error Analysis:\")\n",
    "        # Calculate overall accuracy\n",
    "        accuracy = (pred_labels == true_labels).mean()\n",
    "        print(f\"• Overall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Calculate per-class error rates\n",
    "        per_class_errors = {}\n",
    "        for class_idx, class_name in enumerate(label_names):\n",
    "            class_mask = true_labels == class_idx\n",
    "            if np.any(class_mask):\n",
    "                class_predictions = pred_labels[class_mask]\n",
    "                error_rate = (class_predictions != class_idx).mean()\n",
    "                per_class_errors[class_name] = error_rate\n",
    "\n",
    "        print(\"• Per-Class Error Rates:\")\n",
    "        for class_name, rate in per_class_errors.items():\n",
    "            print(f\"  - {class_name}: {rate:.4f}\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Error: Results for model '{model_name}' not found. Please ensure the model was trained and the results files exist.\"\n",
    "        )\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        \"Error: Could not find 'model_results.json' or 'detailed_results.pkl'. Please ensure the model training and evaluation step was completed successfully.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
