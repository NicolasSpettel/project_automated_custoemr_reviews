{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product categorization and insights using zero-shot classification with bge-reranker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tries to simplify the dataset by clustering it into 6 categories using a<br>\n",
    "zero-shot attempt with the bge-reranker model and give valuable insights into the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import re\n",
    "from collections import Counter\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"/content/Datafiniti_with_sentiments.csv\"\n",
    "df = pd.read_csv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_features(row: pd.Series) -> str:\n",
    "    \"\"\"Extract and clean the product name from a DataFrame row.\"\"\"\n",
    "    name = row.get(\"name\", \"\")\n",
    "    if pd.notna(name) and str(name).strip():\n",
    "        return str(name).strip().lower()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"product_name_clean\"] = df.apply(create_text_features, axis=1)\n",
    "df = df[df[\"product_name_clean\"].str.len() > 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_MAPPING = {\"Positive\": 1.0, \"Neutral\": 0.5, \"Negative\": 0.0}\n",
    "if \"predicted_sentiment_roberta\" in df.columns:\n",
    "    df[\"predicted_sentiment_roberta\"] = df[\"predicted_sentiment_roberta\"].map(\n",
    "        SENTIMENT_MAPPING\n",
    "    )\n",
    "else:\n",
    "    df[\"predicted_sentiment_roberta\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset size after cleaning: {len(df)} reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Product Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "    \"Fire Tablet Special\",\n",
    "    \"AmazonBasics Performance Alkaline(Batteries)\",\n",
    "    \"Anon/Uncategorized\",\n",
    "    \"Echo White Amazon\",\n",
    "    \"Fire Kids Edition\",\n",
    "    \"Fire Amazon\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_reranker(\n",
    "    texts: list[str], labels: list[str], model_name: str\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Classify texts into the most relevant label using a CrossEncoder reranker model.\"\"\"\n",
    "    model = CrossEncoder(model_name)\n",
    "    pairs = [(text, label) for text in texts for label in labels]\n",
    "    scores = model.predict(pairs).reshape(len(texts), len(labels))\n",
    "    best_indices = np.argmax(scores, axis=1)\n",
    "    pred_labels = [labels[i] for i in best_indices]\n",
    "    pred_scores = [scores[i, best_indices[i]] for i in range(len(texts))]\n",
    "    return pred_labels, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_category_stats(data_df: pd.DataFrame, cat_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Compute summary statistics for each category.\"\"\"\n",
    "    stats = []\n",
    "    for cat, group in data_df.groupby(cat_col):\n",
    "        row = {\n",
    "            \"category\": cat,\n",
    "            \"count\": len(group),\n",
    "            \"avg_sentiment\": group[\"predicted_sentiment_roberta\"].mean(),\n",
    "            \"positive_pct\": (group[\"predicted_sentiment_roberta\"] > 0.5).mean() * 100,\n",
    "            \"avg_confidence\": group[\"zero_shot_score\"].mean(),\n",
    "        }\n",
    "        if \"rating\" in group.columns:\n",
    "            row[\"avg_rating\"] = group[\"rating\"].mean()\n",
    "            row[\"high_rating_pct\"] = (group[\"rating\"] >= 4).mean() * 100\n",
    "        if \"doRecommend\" in group.columns:\n",
    "            row[\"recommend_pct\"] = group[\"doRecommend\"].mean() * 100\n",
    "        stats.append(row)\n",
    "    return (\n",
    "        pd.DataFrame(stats).sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, confidence_scores = classify_with_reranker(\n",
    "    df[\"product_name_clean\"].tolist(),\n",
    "    labels=LABELS,\n",
    "    model_name=\"BAAI/bge-reranker-v2-m3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"zero_shot_label\"] = predictions\n",
    "df[\"zero_shot_score\"] = confidence_scores\n",
    "category_results = compute_category_stats(df, \"zero_shot_label\")\n",
    "category_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"category\", y=\"count\", data=category_results)\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.title(\"Review Distribution per Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"category\", y=\"avg_sentiment\", data=category_results)\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.title(\"Average Sentiment per Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive into Product Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products = df[\"product_name_clean\"].value_counts().head(3)\n",
    "print(\"Top 3 Products:\")\n",
    "print(top_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products_stats = (\n",
    "    df[df[\"product_name_clean\"].isin(top_products.index)]\n",
    "    .groupby(\"product_name_clean\")\n",
    "    .agg({\n",
    "        \"predicted_sentiment_roberta\": \"mean\",\n",
    "        \"rating\": \"mean\" if \"rating\" in df.columns else \"first\",\n",
    "        \"doRecommend\": \"mean\" if \"doRecommend\" in df.columns else \"first\",\n",
    "        \"zero_shot_label\": \"first\",\n",
    "    })\n",
    ")\n",
    "top_products_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_complaints(subset: pd.DataFrame, top_n: int = 10) -> list:\n",
    "    \"\"\"Extract the top N most common words from negative reviews.\"\"\"\n",
    "    texts = \" \".join(\n",
    "        subset.loc[subset[\"predicted_sentiment_roberta\"] == 0.0, \"text\"]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "    )\n",
    "    words = re.findall(r\"\\b\\w{3,}\\b\", texts.lower())\n",
    "    return Counter(words).most_common(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in top_products.index:\n",
    "    product_complaints = extract_complaints(df[df[\"product_name_clean\"] == product])\n",
    "    print(f\"\\nTop complaints for {product}:\")\n",
    "    print(product_complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the Worst Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_products = (\n",
    "    df.groupby([\"zero_shot_label\", \"product_name_clean\"])\n",
    "    .agg(\n",
    "        avg_sentiment=(\"predicted_sentiment_roberta\", \"mean\"),\n",
    "        count=(\"product_name_clean\", \"size\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values([\"zero_shot_label\", \"avg_sentiment\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_by_category = worst_products.groupby(\"zero_shot_label\").first().reset_index()\n",
    "print(\"Worst products per category:\")\n",
    "worst_by_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Logging<br>\n",
    "Log to MLflow<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is commented out to prevent running without a configured MLflow server.<br>\n",
    "# !mlflow ui --port 5000 &<br>\n",
    "# from pyngrok import ngrok<br>\n",
    "# from getpass import getpass<br>\n",
    "# ngrok.kill()<br>\n",
    "# NGROK_AUTH_TOKEN = getpass(\"Enter your ngrok authtoken: \")<br>\n",
    "# ngrok.set_auth_token(NGROK_AUTH_TOKEN)<br>\n",
    "# public_url = ngrok.connect(addr=\"5000\", proto=\"http\")<br>\n",
    "# print(\"MLflow Tracking UI:\", public_url)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m mlflow.set_experiment(\u001b[33m\"\u001b[39m\u001b[33mProduct Categorization with Reranker\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'mlflow' is not defined"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Product Categorization with Reranker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"zero_shot_reranker\"):\n",
    "    mlflow.log_param(\"model\", \"BAAI/bge-reranker-v2-m3\")\n",
    "    mlflow.log_param(\"candidate_labels\", LABELS)\n",
    "    for _, result_row in category_results.iterrows():\n",
    "        sanitized_category = re.sub(\n",
    "            r\"[^a-zA-Z0-9_.-]\", \"_\", str(result_row[\"category\"]).replace(\" \", \"_\")\n",
    "        )\n",
    "        prefix = f\"cat_{sanitized_category}\"\n",
    "        mlflow.log_metric(f\"{prefix}_count\", int(result_row[\"count\"]))\n",
    "        if not np.isnan(result_row[\"avg_sentiment\"]):\n",
    "            mlflow.log_metric(\n",
    "                f\"{prefix}_avg_sentiment\", float(result_row[\"avg_sentiment\"])\n",
    "            )\n",
    "            mlflow.log_metric(\n",
    "                f\"{prefix}_positive_pct\", float(result_row[\"positive_pct\"])\n",
    "            )\n",
    "        if not np.isnan(result_row[\"avg_confidence\"]):\n",
    "            mlflow.log_metric(\n",
    "                f\"{prefix}_avg_confidence\", float(result_row[\"avg_confidence\"])\n",
    "            )\n",
    "        if \"avg_rating\" in result_row and not pd.isna(result_row[\"avg_rating\"]):\n",
    "            mlflow.log_metric(f\"{prefix}_avg_rating\", float(result_row[\"avg_rating\"]))\n",
    "        if \"high_rating_pct\" in result_row and not pd.isna(\n",
    "            result_row[\"high_rating_pct\"]\n",
    "        ):\n",
    "            mlflow.log_metric(\n",
    "                f\"{prefix}_high_rating_pct\", float(result_row[\"high_rating_pct\"])\n",
    "            )\n",
    "        if \"recommend_pct\" in result_row and not pd.isna(result_row[\"recommend_pct\"]):\n",
    "            mlflow.log_metric(\n",
    "                f\"{prefix}_recommend_pct\", float(result_row[\"recommend_pct\"])\n",
    "            )\n",
    "    df.to_csv(\"reranker_products.csv\", index=False)\n",
    "    category_results.to_csv(\"reranker_stats.csv\", index=False)\n",
    "    mlflow.log_artifact(\"reranker_products.csv\")\n",
    "    mlflow.log_artifact(\"reranker_stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
