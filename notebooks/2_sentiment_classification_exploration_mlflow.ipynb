{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification and Model Selection with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import os\n",
    "import shutil\n",
    "import threading\n",
    "from typing import Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from pyngrok import ngrok\n",
    "import optuna\n",
    "import torch\n",
    "import nltk\n",
    "from google.colab import files\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow database creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGROK_TOKEN = \"\"\n",
    "ngrok.set_auth_token(NGROK_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_mlflow_server():\n",
    "    subprocess.Popen([\n",
    "        \"mlflow\", \"server\",\n",
    "        \"--backend-store-uri\", \"sqlite:///mlflow.db\",\n",
    "        \"--default-artifact-root\", \"./mlruns\",\n",
    "        \"--host\", \"0.0.0.0\",\n",
    "        \"--port\", \"5000\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_thread = threading.Thread(target=start_mlflow_server)\n",
    "server_thread.start()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_url = ngrok.connect(5000, \"http\")\n",
    "print(\"MLflow Tracking UI is available at:\", public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"sentiment_analysis_experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE_TRAIN = 4\n",
    "BATCH_SIZE_EVAL = 8\n",
    "EPOCHS = 1\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_LENGTH = 256\n",
    "WARMUP_STEPS = 50\n",
    "LR_SCHEDULER = \"linear\"\n",
    "GRADIENT_CHECKPOINTING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ARGS = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_SIZE_EVAL,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    seed=42,\n",
    "    gradient_checkpointing=GRADIENT_CHECKPOINTING,\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(filepath: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(filepath)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    def clean_text(text: str) -> str:\n",
    "        if not isinstance(text, str) or pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        words = word_tokenize(text)\n",
    "        return \" \".join([w for w in words if w not in stop_words and len(w) > 1])\n",
    "    df[\"full_review\"] = df.apply(\n",
    "        lambda row: f\"{str(row.get('title', ''))} {str(row.get('text', ''))}\".strip(),\n",
    "        axis=1,\n",
    "    )\n",
    "    df[\"cleaned_review\"] = df[\"full_review\"].apply(clean_text)\n",
    "    df = df[(df[\"cleaned_review\"].str.len() > 10) & (df[\"star_sentiment\"].notna())]\n",
    "    sentiment_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "    df[\"label\"] = df[\"star_sentiment\"].map(sentiment_map)\n",
    "    samples_per_class = {\"Negative\": 2000, \"Neutral\": 2000, \"Positive\": 4000}\n",
    "    balanced_dfs = []\n",
    "    for sentiment, class_id in sentiment_map.items():\n",
    "        class_df = df[df[\"label\"] == class_id]\n",
    "        n_samples = min(samples_per_class[sentiment], len(class_df))\n",
    "        balanced_dfs.append(class_df.sample(n=n_samples, random_state=42))\n",
    "    return pd.concat(balanced_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_and_model(model_name: str) -> Tuple[AutoTokenizer, AutoModelForSequenceClassification]:\n",
    "    \"\"\"Get tokenizer and model - single source of truth\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=3,\n",
    "        id2label={0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"},\n",
    "        label2id={\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2},\n",
    "    )\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_df: pd.DataFrame, model_tokenizer) -> Dataset:\n",
    "    \"\"\"Convert DataFrame to tokenized Dataset\"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        return model_tokenizer(\n",
    "            examples[\"cleaned_review\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "        )\n",
    "    dataset = Dataset.from_pandas(data_df[[\"cleaned_review\", \"label\"]])\n",
    "    dataset = dataset.map(tokenize_function, batched=True)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics function for trainer\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(model_name_param: str, train_data, eval_data) -> Dict:\n",
    "    \"\"\"Train a single model and return results\"\"\"\n",
    "    print(f\"Training {model_name_param}...\")\n",
    "    with mlflow.start_run(run_name=model_name_param.replace(\"/\", \"_\")):\n",
    "        model_tokenizer, model = get_tokenizer_and_model(model_name_param)\n",
    "        training_args = TrainingArguments(**TRAINING_ARGS.to_dict())\n",
    "        training_args.output_dir = f\"./results/{model_name_param.replace('/', '_')}\"\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=eval_data,\n",
    "            tokenizer=model_tokenizer,\n",
    "        )\n",
    "        trainer.train()\n",
    "        mlflow.pytorch.log_state_dict(trainer.model.state_dict(), artifact_path=\"model\")\n",
    "        predictions = trainer.predict(eval_data)\n",
    "        pred_labels = predictions.predictions.argmax(axis=1)\n",
    "        true_labels = predictions.label_ids\n",
    "        f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "        accuracy = accuracy_score(true_labels, pred_labels)\n",
    "        mlflow.log_params({\n",
    "            \"model_name\": model_name_param,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"batch_size_train\": BATCH_SIZE_TRAIN,\n",
    "            \"batch_size_eval\": BATCH_SIZE_EVAL,\n",
    "            \"epochs\": EPOCHS,\n",
    "        })\n",
    "        mlflow.log_metrics({\"f1_score\": f1, \"accuracy\": accuracy})\n",
    "        return {\"model_name\": model_name_param, \"f1_score\": f1, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"/content/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_clean_data(FILE_PATH)\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model_name in MODELS:\n",
    "    print(f\"Preparing dataset for model: {model_name}\")\n",
    "    \n",
    "    tokenizer, _ = get_tokenizer_and_model(model_name)\n",
    "    train_dataset = prepare_dataset(train_df, tokenizer)\n",
    "    eval_dataset = prepare_dataset(eval_df, tokenizer)\n",
    "    \n",
    "    results[model_name] = train_single_model(model_name, train_dataset, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining completed. Check MLflow UI for results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY OF ALL MODEL RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<50} {'F1 Score':<10} {'Accuracy':<10}\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, result in results.items():\n",
    "    if \"error\" not in result:\n",
    "        print(f\"{model_name:<50} {result['f1_score']:<10.4f} {result['accuracy']:<10.4f}\")\n",
    "    else:\n",
    "        print(f\"{model_name:<50} {'ERROR':<10} {'ERROR':<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_results = {k: v for k, v in results.items() if \"error\" not in v}\n",
    "if valid_results:\n",
    "    best_model = max(valid_results.items(), key=lambda x: x[1][\"f1_score\"])\n",
    "    print(f\"\\nBest Model: {best_model[0]} (F1: {best_model[1]['f1_score']:.4f}, Accuracy: {best_model[1]['accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(optuna_trial):\n",
    "    \"\"\"Optuna objective function for hyperparameter optimization\"\"\"\n",
    "    with mlflow.start_run():\n",
    "        learning_rate = optuna_trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "        per_device_train_batch_size = optuna_trial.suggest_categorical(\"train_batch_size\", [8, 16])\n",
    "        num_train_epochs = optuna_trial.suggest_int(\"epochs\", 2, 3)\n",
    "        weight_decay = 0.1\n",
    "        lr_scheduler_type = \"cosine\"\n",
    "\n",
    "        # Reuse existing data splits\n",
    "        opt_tokenizer, opt_model = get_tokenizer_and_model(MODEL_NAME)\n",
    "        opt_train_dataset = prepare_dataset(train_df, opt_tokenizer)\n",
    "        opt_eval_dataset = prepare_dataset(eval_df, opt_tokenizer)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/{MODEL_NAME.replace('/', '_')}_optuna_trial_{optuna_trial.number}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1,\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            weight_decay=weight_decay,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            warmup_steps=500,\n",
    "            logging_steps=100,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            report_to=\"none\",\n",
    "            disable_tqdm=True,\n",
    "            seed=42,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=opt_model,\n",
    "            args=training_args,\n",
    "            train_dataset=opt_train_dataset,\n",
    "            eval_dataset=opt_eval_dataset,\n",
    "            tokenizer=opt_tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "        mlflow.log_params({\n",
    "            \"trial\": optuna_trial.number,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"train_batch_size\": per_device_train_batch_size,\n",
    "            \"epochs\": num_train_epochs,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"scheduler\": lr_scheduler_type,\n",
    "        })\n",
    "        mlflow.log_metrics({\"f1_score\": metrics[\"eval_f1\"], \"accuracy\": metrics[\"eval_accuracy\"]})\n",
    "        return metrics[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_mlflow(run_id: str, model_name_param: str):\n",
    "    \"\"\"Load model from MLflow run\"\"\"\n",
    "    print(f\"Loading model from MLflow run ID: {run_id}\")\n",
    "    load_tokenizer, load_model = get_tokenizer_and_model(model_name_param)\n",
    "    \n",
    "    model_uri = f\"runs:/{run_id}/model/state_dict.pth\"\n",
    "    state_dict = mlflow.pytorch.load_state_dict(model_uri)\n",
    "    load_model.load_state_dict(state_dict)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return load_model, load_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best trial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain with best parameters for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"best_model_final\"):\n",
    "    best_tokenizer, best_model = get_tokenizer_and_model(MODEL_NAME)\n",
    "    best_train_dataset = prepare_dataset(train_df, best_tokenizer)\n",
    "    best_eval_dataset = prepare_dataset(eval_df, best_tokenizer)\n",
    "    final_training_args = TrainingArguments(\n",
    "        output_dir=\"./results/final_best_model\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        per_device_train_batch_size=best_params[\"train_batch_size\"],\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=best_params[\"epochs\"],\n",
    "        weight_decay=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps=500,\n",
    "        logging_steps=100,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        disable_tqdm=False,\n",
    "        seed=42,\n",
    "    )\n",
    "    final_trainer = Trainer(\n",
    "        model=best_model,\n",
    "        args=final_training_args,\n",
    "        train_dataset=best_train_dataset,\n",
    "        eval_dataset=best_eval_dataset,\n",
    "        tokenizer=best_tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    final_trainer.train()\n",
    "    \n",
    "    # Save the best trained model\n",
    "    final_trainer.save_model(\"./saved_roberta_model\")\n",
    "    best_tokenizer.save_pretrained(\"./saved_roberta_model\")\n",
    "    \n",
    "    print(\"Best model saved to ./saved_roberta_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive('saved_roberta_model', 'zip', './saved_roberta_model')\n",
    "files.download('saved_roberta_model.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"./mlruns\"):\n",
    "    shutil.make_archive('mlflow_complete', 'zip', './', 'mlruns')\n",
    "    files.download('mlflow_complete.zip')\n",
    "    print(\"MLflow data downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"mlflow.db\"):\n",
    "    files.download(\"mlflow.db\")\n",
    "    print(\"MLflow database downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(\"sentiment_analysis_experiment\")\n",
    "    \n",
    "    if experiment:\n",
    "        runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "        \n",
    "        if not runs_df.empty:\n",
    "            print(\"MLflow Runs Summary:\")\n",
    "            print(f\"Total runs: {len(runs_df)}\")\n",
    "            \n",
    "            print(\"\\nAvailable columns:\")\n",
    "            for col in sorted(runs_df.columns):\n",
    "                if not col.startswith('tags.') and not col.startswith('artifact_uri'):\n",
    "                    print(f\"  - {col}\")\n",
    "            \n",
    "            runs_df.to_csv('mlflow_runs_complete.csv', index=False)\n",
    "            files.download('mlflow_runs_complete.csv')\n",
    "            print(\"Complete MLflow runs data downloaded!\")\n",
    "        else:\n",
    "            print(\"No runs found in MLflow\")\n",
    "    else:\n",
    "        print(\"Experiment 'sentiment_analysis_experiment' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "except Exception as e:\n",
    "    print(f\"Error accessing MLflow: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBackup complete! You can now:\")\n",
    "print(\"1. Extract mlflow_complete.zip locally\")\n",
    "print(\"2. Run 'mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns'\")\n",
    "print(\"3. Open http://localhost:5000 to see your dashboard\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
