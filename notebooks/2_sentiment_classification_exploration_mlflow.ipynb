{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlflow pyngrok\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import os\n",
    "import threading\n",
    "from typing import Dict\n",
    "import shutil\n",
    "\n",
    "from google.colab import files, drive\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from pyngrok import ngrok\n",
    "import optuna\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7072329",
   "metadata": {},
   "source": [
    "## MLflow and ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba635b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your ngrok token\n",
    "NGROK_TOKEN = \"\"\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "\n",
    "\n",
    "def start_mlflow_server():\n",
    "    \"\"\"\n",
    "    Starts the MLflow server in a subprocess.\n",
    "    \"\"\"\n",
    "    subprocess.Popen(\n",
    "        [\n",
    "            \"mlflow\",\n",
    "            \"server\",\n",
    "            \"--backend-store-uri\",\n",
    "            \"sqlite:///mlflow.db\",\n",
    "            \"--default-artifact-root\",\n",
    "            \"./mlruns\",\n",
    "            \"--host\",\n",
    "            \"0.0.0.0\",\n",
    "            \"--port\",\n",
    "            \"5000\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "server_thread = threading.Thread(target=start_mlflow_server)\n",
    "server_thread.start()\n",
    "time.sleep(5)\n",
    "\n",
    "public_url = ngrok.connect(5000, \"http\")\n",
    "print(\"MLflow Tracking UI is available at:\", public_url)\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"sentiment_analysis_experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcdb004",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE_TRAIN = 4\n",
    "BATCH_SIZE_EVAL = 8\n",
    "EPOCHS = 1\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_LENGTH = 256\n",
    "WARMUP_STEPS = 50\n",
    "LR_SCHEDULER = \"linear\"\n",
    "GRADIENT_CHECKPOINTING = True\n",
    "\n",
    "TRAINING_ARGS = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_SIZE_EVAL,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    eval_strategy=\"epoch\", # eval_strategy for older version\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    seed=42,\n",
    "    gradient_checkpointing=GRADIENT_CHECKPOINTING,\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")\n",
    "\n",
    "MODELS = [\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d27c071",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40083d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a CSV file, cleans the text data, and balances the dataset.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and balanced DataFrame with 'label' and 'cleaned_review' columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleans a single string of text by lowercasing, removing punctuation,\n",
    "        numbers, extra spaces, and stopwords.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text string.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned text string.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        words = word_tokenize(text)\n",
    "        return \" \".join([w for w in words if w not in stop_words and len(w) > 1])\n",
    "\n",
    "    df[\"full_review\"] = df.apply(\n",
    "        lambda row: f\"{str(row.get('title', ''))} {str(row.get('text', ''))}\".strip(),\n",
    "        axis=1,\n",
    "    )\n",
    "    df[\"cleaned_review\"] = df[\"full_review\"].apply(clean_text)\n",
    "\n",
    "    df = df[(df[\"cleaned_review\"].str.len() > 10) & (df[\"star_sentiment\"].notna())]\n",
    "\n",
    "    sentiment_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "    df[\"label\"] = df[\"star_sentiment\"].map(sentiment_map)\n",
    "\n",
    "    # Balance the dataset by sampling an equal number of reviews from each class\n",
    "    samples_per_class = {\"Negative\": 1000, \"Neutral\": 1000, \"Positive\": 1000}\n",
    "    balanced_dfs = []\n",
    "    for sentiment, class_id in sentiment_map.items():\n",
    "        class_df = df[df[\"label\"] == class_id]\n",
    "        n_samples = min(samples_per_class[sentiment], len(class_df))\n",
    "        balanced_dfs.append(class_df.sample(n=n_samples, random_state=42))\n",
    "\n",
    "    return pd.concat(balanced_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4a8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df: pd.DataFrame, tokenizer) -> Dataset:\n",
    "    \"\"\"\n",
    "    Converts a pandas DataFrame into a Hugging Face Dataset and tokenizes it.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with 'cleaned_review' and 'label' columns.\n",
    "        tokenizer: The tokenizer object from the Hugging Face library.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: A tokenized Hugging Face Dataset.\n",
    "    \"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"cleaned_review\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "        )\n",
    "\n",
    "    dataset = Dataset.from_pandas(df[[\"cleaned_review\", \"label\"]])\n",
    "    dataset = dataset.map(tokenize_function, batched=True)\n",
    "    dataset.set_format(\n",
    "        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8824ea",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(model_name: str, train_dataset, eval_dataset) -> Dict:\n",
    "    \"\"\"\n",
    "    Trains a single transformer model, logs metrics and artifacts with MLflow.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the Hugging Face model to train.\n",
    "        train_dataset (Dataset): The training dataset.\n",
    "        eval_dataset (Dataset): The evaluation dataset.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary containing the model name, F1 score, and accuracy.\n",
    "    \"\"\"\n",
    "    print(f\" Training {model_name}...\")\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name.replace(\"/\", \"_\")):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=3,\n",
    "            id2label={0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"},\n",
    "            label2id={\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2},\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(**TRAINING_ARGS.to_dict())\n",
    "        training_args.output_dir = f\"./results/{model_name.replace('/', '_')}\"\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        mlflow.pytorch.log_state_dict(trainer.model.state_dict(), artifact_path=\"model\")\n",
    "\n",
    "        predictions = trainer.predict(eval_dataset)\n",
    "        pred_labels = predictions.predictions.argmax(axis=1)\n",
    "        true_labels = predictions.label_ids\n",
    "\n",
    "        f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "        accuracy = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "        mlflow.log_params(\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"learning_rate\": LEARNING_RATE,\n",
    "                \"batch_size_train\": BATCH_SIZE_TRAIN,\n",
    "                \"batch_size_eval\": BATCH_SIZE_EVAL,\n",
    "                \"epochs\": EPOCHS,\n",
    "            }\n",
    "        )\n",
    "        mlflow.log_metrics({\"f1_score\": f1, \"accuracy\": accuracy})\n",
    "\n",
    "        return {\"model_name\": model_name, \"f1_score\": f1, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99813d",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"/content/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_cleaned.csv\"\n",
    "df = load_and_clean_data(FILE_PATH)\n",
    "\n",
    "train_df, eval_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "results = {}\n",
    "for model_name in MODELS:\n",
    "    print(f\"Preparing dataset for model: {model_name}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    train_dataset = prepare_dataset(train_df, tokenizer)\n",
    "    eval_dataset = prepare_dataset(eval_df, tokenizer)\n",
    "\n",
    "    results[model_name] = train_single_model(\n",
    "        model_name, train_dataset, eval_dataset\n",
    "    )\n",
    "\n",
    "print(\"\\n Training completed. Check MLflow UI for results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc92ae0",
   "metadata": {},
   "source": [
    "## First Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY OF ALL MODEL RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<50} {'F1 Score':<10} {'Accuracy':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Show model scores\n",
    "for model_name, result in results.items():\n",
    "    if \"error\" not in result:\n",
    "        print(\n",
    "            f\"{model_name:<50} {result['f1_score']:<10.4f} {result['accuracy']:<10.4f}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"{model_name:<50} {'ERROR':<10} {'ERROR':<10}\")\n",
    "\n",
    "# Find the best performing model based on F1 score\n",
    "valid_results = {k: v for k, v in results.items() if \"error\" not in v}\n",
    "if valid_results:\n",
    "    best_model = max(valid_results.items(), key=lambda x: x[1][\"f1_score\"])\n",
    "    print(\n",
    "        f\"\\nBest Model: {best_model[0]} (F1: {best_model[1]['f1_score']:.4f}, Accuracy: {best_model[1]['accuracy']:.4f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc4b7a",
   "metadata": {},
   "source": [
    "## Optuna objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4098523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines the Optuna objective function for hyperparameter optimization.\n",
    "\n",
    "    This function trains a model with a given set of hyperparameters from an\n",
    "    Optuna trial and returns the evaluation F1 score to be maximized.\n",
    "    \"\"\"\n",
    "    # Start a new MLflow run for each trial\n",
    "    with mlflow.start_run():\n",
    "        # Define the hyperparameter search space using trial suggestions\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "        per_device_train_batch_size = trial.suggest_categorical(\n",
    "            \"train_batch_size\", [4, 8, 16]\n",
    "        )\n",
    "        num_train_epochs = trial.suggest_int(\"epochs\", 2, 5)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "        lr_scheduler_type = trial.suggest_categorical(\"scheduler\", [\"linear\", \"cosine\"])\n",
    "\n",
    "        # Prepare the dataset, ensuring the data loading and splitting are within the objective\n",
    "        df = load_and_clean_data(FILE_PATH)\n",
    "        train_df, eval_df = train_test_split(\n",
    "            df, test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        train_dataset = prepare_dataset(train_df, tokenizer)\n",
    "        eval_dataset = prepare_dataset(eval_df, tokenizer)\n",
    "\n",
    "        # Define the model with the appropriate labels\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=3,\n",
    "            id2label={0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"},\n",
    "            label2id={\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2},\n",
    "        )\n",
    "\n",
    "        # Define training arguments for the current trial\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/{MODEL_NAME.replace('/', '_')}_optuna_trial_{trial.number}\",\n",
    "            eval_strategy=\"epoch\", \n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1,\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            weight_decay=weight_decay,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            warmup_steps=500,\n",
    "            logging_steps=100,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            report_to=\"none\",\n",
    "            disable_tqdm=True,\n",
    "            seed=42,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "        )\n",
    "\n",
    "        # Define the compute metrics function\n",
    "        def compute_metrics(eval_pred):\n",
    "            \"\"\"Computes accuracy and weighted F1 score from predictions.\"\"\"\n",
    "            logits, labels = eval_pred\n",
    "            preds = logits.argmax(axis=-1)\n",
    "            return {\n",
    "                \"accuracy\": accuracy_score(labels, preds),\n",
    "                \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "            }\n",
    "\n",
    "        # Initialize the Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "\n",
    "        # Log hyperparameters and metrics to MLflow for the current trial\n",
    "        mlflow.log_params(\n",
    "            {\n",
    "                \"trial\": trial.number,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"train_batch_size\": per_device_train_batch_size,\n",
    "                \"epochs\": num_train_epochs,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"scheduler\": lr_scheduler_type,\n",
    "            }\n",
    "        )\n",
    "        mlflow.log_metrics(\n",
    "            {\"f1_score\": metrics[\"eval_f1\"], \"accuracy\": metrics[\"eval_accuracy\"]}\n",
    "        )\n",
    "\n",
    "        # Return the F1 score to Optuna\n",
    "        return metrics[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2aa90",
   "metadata": {},
   "source": [
    "## Run Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal model\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "# End any existing active run before starting the study\n",
    "mlflow.end_run() \n",
    "# Create and run the Optuna study to find the best hyperparameters\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print the best trial's parameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model():\n",
    "    \"\"\"Find and load the best model from results directory.\"\"\"\n",
    "    \n",
    "    results_dir = \"./results\"\n",
    "    best_trial_num = -1\n",
    "    best_model_path = None\n",
    "    \n",
    "    print(f\"Scanning {results_dir}...\")\n",
    "    \n",
    "    # Find the highest trial number with a best_model directory\n",
    "    for trial_dir in os.listdir(results_dir):\n",
    "        if \"optuna_trial_\" in trial_dir:\n",
    "            trial_num = int(trial_dir.split(\"_trial_\")[-1])\n",
    "            trial_path = os.path.join(results_dir, trial_dir)\n",
    "            best_model_dir = os.path.join(trial_path, \"best_model\")\n",
    "            \n",
    "            if os.path.exists(best_model_dir) and trial_num > best_trial_num:\n",
    "                best_trial_num = trial_num\n",
    "                best_model_path = best_model_dir\n",
    "                print(f\"Found trial {trial_num} with best_model\")\n",
    "    \n",
    "    if best_model_path is None:\n",
    "        raise ValueError(\"No best_model directories found!\")\n",
    "    \n",
    "    print(f\"Loading from trial {best_trial_num}: {best_model_path}\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489094b7",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, texts):\n",
    "    \"\"\"Predict sentiment for given texts.\"\"\"\n",
    "    inputs = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "    return [labels[pred] for pred in predictions]\n",
    "\n",
    "model, tokenizer = load_best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef5304",
   "metadata": {},
   "source": [
    "## Save & Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./saved_roberta_model\")\n",
    "tokenizer.save_pretrained(\"./saved_roberta_model\")\n",
    "\n",
    "shutil.make_archive('saved_roberta_model', 'zip', './saved_roberta_model')\n",
    "files.download('saved_roberta_model.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd58e62",
   "metadata": {},
   "source": [
    "## Back up for MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02152b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Backup the MLflow database and runs\n",
    "if os.path.exists(\"./mlruns\"):\n",
    "    shutil.make_archive('mlflow_complete', 'zip', './', 'mlruns')\n",
    "    files.download('mlflow_complete.zip')\n",
    "    print(\" MLflow data downloaded!\")\n",
    "\n",
    "# 2. Also backup the SQLite database if it exists\n",
    "if os.path.exists(\"mlflow.db\"):\n",
    "    files.download(\"mlflow.db\")\n",
    "    print(\" MLflow database downloaded!\")\n",
    "\n",
    "# 3. Create a simple summary from your actual MLflow runs\n",
    "\n",
    "\n",
    "try:\n",
    "    # Get the experiment you set up\n",
    "    experiment = mlflow.get_experiment_by_name(\"sentiment_analysis_experiment\")\n",
    "    \n",
    "    if experiment:\n",
    "        runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "        \n",
    "        if not runs_df.empty:\n",
    "            print(\"\\n MLflow Runs Summary:\")\n",
    "            print(f\"Total runs: {len(runs_df)}\")\n",
    "            \n",
    "            # Show available columns\n",
    "            print(\"\\nAvailable columns:\")\n",
    "            for col in sorted(runs_df.columns):\n",
    "                if not col.startswith('tags.') and not col.startswith('artifact_uri'):\n",
    "                    print(f\"  - {col}\")\n",
    "            \n",
    "            # Save the complete runs data\n",
    "            runs_df.to_csv('mlflow_runs_complete.csv', index=False)\n",
    "            files.download('mlflow_runs_complete.csv')\n",
    "            print(\" Complete MLflow runs data downloaded!\")\n",
    "            \n",
    "        else:\n",
    "            print(\" No runs found in MLflow\")\n",
    "    else:\n",
    "        print(\" Experiment 'sentiment_analysis_experiment' not found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Error accessing MLflow: {e}\")\n",
    "\n",
    "print(\"\\n Backup complete! You can now:\")\n",
    "print(\"1. Extract mlflow_complete.zip locally\")  \n",
    "print(\"2. Run 'mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns'\")\n",
    "print(\"3. Open http://localhost:5000 to see your dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ff2a2",
   "metadata": {},
   "source": [
    "## Load best config and make final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98560dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at C:\\Users\\nicol\\Desktop\\Ironhack\\week6\\Project_NLP\\models\\roberta-latest and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_directory = r\"C:\\Users\\nicol\\Desktop\\Ironhack\\week6\\Project_NLP\\models\\roberta-latest\"\n",
    "\n",
    "try:\n",
    "    # Attempt to load the tokenizer and model\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_directory)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_directory)\n",
    "\n",
    "    print(\"Model and tokenizer loaded successfully!\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load your dataset ---\n",
    "df = pd.read_csv(\"/content/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_cleaned.csv\")\n",
    "df[\"combined_review\"] = df[\"title\"].astype(str) + \" \" + df[\"text\"].astype(str)\n",
    "\n",
    "# Map labels to integers: Negative=0, Neutral=1, Positive=2\n",
    "label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "df[\"labels\"] = df[\"star_sentiment\"].map(label_map)\n",
    "\n",
    "# Convert to HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# --- Load model + tokenizer ---\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "\n",
    "# --- Tokenization function ---\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"combined_review\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Split dataset\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# --- Training arguments with best parameters ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2.9443750826822793e-05,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.09960856196239078,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# --- Define Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ec145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Save final model ---\n",
    "trainer.save_model(\"./drive/MyDrive/finetuned_roberta_sentiment\")\n",
    "tokenizer.save_pretrained(\"./drive/MyDrive/finetuned_roberta_sentiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
