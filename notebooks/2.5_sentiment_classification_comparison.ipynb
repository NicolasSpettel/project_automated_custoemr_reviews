{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification comparison between RoBERTa and ChatGPT models.<br>\n",
    "\n",
    "This notebook evaluates the performance of the fine-tuned RoBERTa model against<br>\n",
    "OpenAI's **ChatGPT** (specifically, gpt-3.5-turbo) on a sampled subset of the<br>\n",
    "Amazon reviews. It includes a series of visualizations and a summary of key metrics.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Sentiment Prediction and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../data/processed/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_cleaned.csv\"\n",
    ")\n",
    "df['combined_review'] = df['title'] + \" \" + df['text']\n",
    "MODEL_DIR = (\n",
    "    r\"C:\\Users\\nicol\\Desktop\\Ironhack\\week6\\Project_NLP\\models\\cardiffnlptwitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "config = AutoConfig.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_roberta = []\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch processing for efficiency on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "for batch_idx in range(0, len(df), BATCH_SIZE):\n",
    "    batch = df['combined_review'].iloc[batch_idx:batch_idx + BATCH_SIZE].tolist()\n",
    "\n",
    "    # Ensure all reviews in the batch are strings\n",
    "    batch = [str(review) if not pd.isna(review) else \"\" for review in batch]\n",
    "    inputs = tokenizer(\n",
    "        batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_ids = logits.argmax(dim=-1).cpu().numpy()\n",
    "    predicted_labels = [\n",
    "        model.config.id2label.get(id, str(id)) for id in predicted_class_ids\n",
    "    ]\n",
    "    predictions_roberta.extend(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_sentiment_roberta'] = predictions_roberta\n",
    "df['predicted_sentiment_roberta'] = df['predicted_sentiment_roberta'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the updated DataFrame to a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE_PATH = \"../data/processed/Datafiniti_with_sentiments.csv\"\n",
    "df.to_csv(OUTPUT_FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization & Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"Class for analyzing and comparing sentiment models.\"\"\"\n",
    "    def __init__(self, data_df):\n",
    "        self.df = data_df.copy()\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "    def create_subset(self, n_per_class=20):\n",
    "        \"\"\"Create balanced subset for comparison.\"\"\"\n",
    "        subset_list = []\n",
    "        for sentiment in ['positive', 'negative', 'neutral']:\n",
    "            class_data = self.df[self.df['star_sentiment'].str.lower() == sentiment]\n",
    "            if len(class_data) >= n_per_class:\n",
    "                subset_list.append(class_data.sample(n=n_per_class, random_state=42))\n",
    "        return pd.concat(subset_list, ignore_index=True)\n",
    "    def get_chatgpt_prediction(self, text):\n",
    "        \"\"\"Get ChatGPT prediction for sentiment.\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\"You are a sentiment classifier. Respond with EXACTLY \"\n",
    "                               \"one word: Positive, Negative, or Neutral. No other text.\")\n",
    "                }, {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Classify this sentiment: {text[:500]}\"\n",
    "                }],\n",
    "                max_tokens=1,\n",
    "                temperature=0\n",
    "            )\n",
    "            result = response.choices[0].message.content.strip()\n",
    "            if result in ['Positive', 'Negative', 'Neutral']:\n",
    "                return result\n",
    "            return 'Neutral'\n",
    "        except Exception:\n",
    "            return 'Neutral'\n",
    "    def analyze_chatgpt_subset(self, subset_df):\n",
    "        \"\"\"Analyze subset with ChatGPT predictions.\"\"\"\n",
    "        chatgpt_preds = []\n",
    "        for _, row in subset_df.iterrows():\n",
    "            text = str(row.get('combined_review', row.get('text', '')))\n",
    "            pred = self.get_chatgpt_prediction(text)\n",
    "            chatgpt_preds.append(pred)\n",
    "            if len(chatgpt_preds) % 10 == 0:\n",
    "                print(f\"Processed {len(chatgpt_preds)}/{len(subset_df)}\")\n",
    "            time.sleep(0.5)\n",
    "        subset_df = subset_df.copy()\n",
    "        subset_df['chatgpt_prediction'] = chatgpt_preds\n",
    "        return subset_df\n",
    "    def plot_roberta_analysis(self):\n",
    "        \"\"\"Plot RoBERTa analysis results.\"\"\"\n",
    "        df_clean = self.df.dropna(subset=['star_sentiment', 'predicted_sentiment_roberta'])\n",
    "        df_clean['star_sentiment'] = df_clean['star_sentiment'].str.lower()\n",
    "        df_clean['predicted_sentiment_roberta'] = (\n",
    "            df_clean['predicted_sentiment_roberta'].str.lower()\n",
    "        )\n",
    "        y_true = df_clean['star_sentiment']\n",
    "        y_pred = df_clean['predicted_sentiment_roberta']\n",
    "        labels = ['negative', 'neutral', 'positive']\n",
    "        _, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        plt.suptitle('RoBERTa vs True Labels Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                   xticklabels=labels, yticklabels=labels, ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Confusion Matrix')\n",
    "\n",
    "        # Accuracy by class\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, labels=labels\n",
    "        )\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.25\n",
    "        axes[0, 1].bar(x - width, precision, width, label='Precision', alpha=0.8)\n",
    "        axes[0, 1].bar(x, recall, width, label='Recall', alpha=0.8)\n",
    "        axes[0, 1].bar(x + width, f1, width, label='F1-Score', alpha=0.8)\n",
    "        axes[0, 1].set_xticks(x)\n",
    "        axes[0, 1].set_xticklabels(labels)\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].set_title('Performance by Class')\n",
    "\n",
    "        # Distribution comparison\n",
    "        true_counts = y_true.value_counts()\n",
    "        pred_counts = y_pred.value_counts()\n",
    "        x = np.arange(len(labels))\n",
    "        axes[1, 0].bar(\n",
    "            x - 0.2, [true_counts.get(l, 0) for l in labels], 0.4,\n",
    "            label='True', alpha=0.8\n",
    "        )\n",
    "        axes[1, 0].bar(\n",
    "            x + 0.2, [pred_counts.get(l, 0) for l in labels], 0.4,\n",
    "            label='Predicted', alpha=0.8\n",
    "        )\n",
    "        axes[1, 0].set_xticks(x)\n",
    "        axes[1, 0].set_xticklabels(labels)\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].set_title('Distribution Comparison')\n",
    "\n",
    "        # Overall accuracy\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        axes[1, 1].bar(['RoBERTa'], [accuracy], color='green', alpha=0.7)\n",
    "        axes[1, 1].set_ylabel('Accuracy')\n",
    "        axes[1, 1].set_title(f'Overall Accuracy: {accuracy:.3f}')\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"RoBERTa Accuracy: {accuracy:.3f}\")\n",
    "        print(classification_report(y_true, y_pred, target_names=labels))\n",
    "        return accuracy\n",
    "    def compare_models(self, comparison_subset):\n",
    "        \"\"\"Compare RoBERTa and ChatGPT models.\"\"\"\n",
    "        df_clean = comparison_subset.dropna()\n",
    "        df_clean['star_sentiment'] = df_clean['star_sentiment'].str.lower()\n",
    "        df_clean['predicted_sentiment_roberta'] = (\n",
    "            df_clean['predicted_sentiment_roberta'].str.lower()\n",
    "        )\n",
    "        df_clean['chatgpt_prediction'] = df_clean['chatgpt_prediction'].str.lower()\n",
    "        y_true = df_clean['star_sentiment']\n",
    "        y_roberta = df_clean['predicted_sentiment_roberta']\n",
    "        y_chatgpt = df_clean['chatgpt_prediction']\n",
    "        roberta_accuracy = accuracy_score(y_true, y_roberta)\n",
    "        chatgpt_accuracy = accuracy_score(y_true, y_chatgpt)\n",
    "        _, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        # Accuracy comparison\n",
    "        models = ['ChatGPT', 'RoBERTa']\n",
    "        accuracies = [chatgpt_accuracy, roberta_accuracy]\n",
    "        axes[0].bar(models, accuracies, color=['#FF6B6B', '#4ECDC4'], alpha=0.8)\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].set_title('Model Comparison')\n",
    "        axes[0].set_ylim(0, 1)\n",
    "        for idx, v in enumerate(accuracies):\n",
    "            axes[0].text(idx, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "        # ChatGPT confusion matrix\n",
    "        labels = ['positive', 'negative', 'neutral']\n",
    "        cm_chatgpt = confusion_matrix(y_true, y_chatgpt, labels=labels)\n",
    "        sns.heatmap(cm_chatgpt, annot=True, fmt='d', cmap='Reds',\n",
    "                   xticklabels=labels, yticklabels=labels, ax=axes[1])\n",
    "        axes[1].set_title('ChatGPT Confusion Matrix')\n",
    "\n",
    "        # RoBERTa confusion matrix\n",
    "        cm_roberta = confusion_matrix(y_true, y_roberta, labels=labels)\n",
    "        sns.heatmap(cm_roberta, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=labels, yticklabels=labels, ax=axes[2])\n",
    "        axes[2].set_title('RoBERTa Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"ChatGPT Accuracy: {chatgpt_accuracy:.3f}\")\n",
    "        print(f\"RoBERTa Accuracy: {roberta_accuracy:.3f}\")\n",
    "        print(f\"Agreement Rate: {(y_chatgpt == y_roberta).mean():.3f}\")\n",
    "        return chatgpt_accuracy, roberta_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/Datafiniti_with_sentiments.csv\")\n",
    "analyzer = SentimentAnalyzer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_accuracy = analyzer.plot_roberta_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = analyzer.create_subset(n_per_class=20)\n",
    "subset_with_chatgpt = analyzer.analyze_chatgpt_subset(subset)\n",
    "chatgpt_acc, roberta_acc = analyzer.compare_models(subset_with_chatgpt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
